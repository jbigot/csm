%----------------------------------------
\subsection{Multi Stencil Language}
%----------------------------------------

\begin{figure}[t]
\begin{lstlisting}[basicstyle=\small,mathescape,frame=single,language=C++,numbers=left]
program ::= "mesh:" meshid 
            "mesh entities:" listmeshent
            "computation domains:" 
                       listcompdom
            "independent:"
                       listinde
            "data:" listdata
            "time:" iteration
            "computations:" listcomp
listmeshent ::= meshent listmeshent
             |  meshent
listcompdom ::= compdom listcompdom
             |  compdom
compdom ::= compdomid "in" listmeshent
listinde ::= inde listinde
          |  inde
inde ::= compdomid "and" compdomid
listdata ::= data listdata
          |  data
data ::= dataid "," meshent
iteration ::= num
listcomp ::= comp listcomp
          |  comp
comp ::= dataid "[" compdomid "]=" compid "(" 
            listdataread ")"
listdataread ::= dataread listdataread
              |  dataread
dataread ::= dataid "[" neighborid "]"
          |  dataid
\end{lstlisting}
\caption{Grammar of the MS language}
\label{fig:grammar}
\end{figure}

As mentioned in the previous section, the Multi Stencil Language (MSL) is an agnostic descriptive language for multi-stencil simulations. Six main sections are required in a MSL description that match the six tuples of the formal definition of a multi-stencil program: \textbf{1.}\,the mesh description, \textbf{2.}\,the mesh entities description, \textbf{3.}\,the computation domains description and their dependencies, \textbf{5.}\,the data description, \textbf{5.}\,the time loop description, and finally \textbf{6.} the computations description.
Figure~\ref{fig:grammar} shows the grammar of MSL. Lines 1 to 9 define what is a MSL program with the different parts mentioned above. The remaining of this section describes those different parts of a MSL program and presents an example.
We can note that, as MSL is a descriptive language, almost all terminals \texttt{meshid}, \texttt{meshdom}, \texttt{compdomid}, \texttt{dataid}, \texttt{compid} and \texttt{neighborid}, are string identifiers. Only \texttt{num} is an integer terminal.

\subsubsection*{Mesh and Mesh Entities}
As illustrated in the first line of the grammar in Figure~\ref{fig:grammar}, a mesh is simply defined by a string identifier. Then, mesh entities are described as a list of identifiers (lines 2, 10 and 11 in Figure~\ref{fig:grammar}). Figure~\ref{fig:mslex} presents an example defining a Cartesian mesh, called \texttt{cart}, and three mesh entities, called \texttt{cell}, \texttt{edgex} and \texttt{edgey}.

\subsubsection*{Computation Domains and Their Dependencies}
A computation domain can be seen as a subpart of entities of a mesh which will be used during one or more computation. For each computation domain, the mesh entity from which it is built is indicated (lines 3-4 and 12-14 in Figure~\ref{fig:grammar}). By default two computation domains always intersect except when an independence relation is mentioned in the section \emph{independent} of the language (lines 5-6 and 15-17 in Figure~\ref{fig:grammar}).
In the lines 4 to 6 of Figure~\ref{fig:mslex}, computation domains are defined for each mesh entity, called \texttt{allcell}, \texttt{alledgex} and \texttt{alledgey}; they represent the whole domain.
On lines 7 and 8 two other subparts of the mesh entity \texttt{edgex} are defined, called \texttt{part1edgex} and \texttt{part2edgex}; they represent the boundaries of the domain.
On line 10 the independence relation between \texttt{part1edgex} and \texttt{part2edgex} is specified since they do not intersect.

\subsubsection*{Data and Time}
A data is a quantity to simulate. It is mapped on mesh entities, as illustrated in lines 7 and 18-20 of Figure~\ref{fig:grammar}. The \texttt{time} section simply indicates a number of iterations to perform in the simulation. In the example of Figure~\ref{fig:mslex} ten data identifiers are defined and 500 iterations (lines 11-22). %An extension could be to support a convergence criterion.


\subsubsection*{Computation Description}
The last part of the language contains the specifications of stencil kernels and local computations as defined in Section~\ref{sect:multistencil}.
This description does not contain a direct expression of the numerical expressions.
Instead, the term $exp$ of Equations~\ref{eq:st} and~\ref{eq:loc} simply corresponds to an identifier that references an implementation in an external language  (\cf Section~\ref{sect:component}).
The specification starts by the identifier of the data to compute ($w$ in the formal description) with the computation domain between brackets ($d$ in the formal definition).
Then, after the equal sign is specified the kernel identifier.
Finally, between the parenthesis is the list of identifiers of data read by the computation with their stencil shape between brackets ($R$ in the formal definition).
If the computation is local, no brackets appear ($R_l$ in the formal definition). In the example of Figure~\ref{fig:mslex}, nine computations are described. On line 24, the data \texttt{b} is computed on the computation domain \texttt{allcell} by the computation \texttt{c\_0} which read data \texttt{a} without neighborhood shape.

\begin{figure}[t]
\begin{lstlisting}[basicstyle=\small,mathescape,frame=single,language=C++,numbers=left]
mesh: cart
mesh domains: cell,edgex,edgey
computation domains:
  allcell in cell
  alledgex in edgex
  alledgey in edgey
  part1edgex in edgex
  part2edgex in edgex
independent:
  part1edgex and part2edgex
data:
  a,cell
  b,cell
  c,edgex
  d,edgex
  e,edgey
  f,cell
  g,edgey
  h,edgex
  i,cell
  j,edgex
time:500
computations:
  b[allcell]=c0(a)
  c[alledgex]=c1(b[n1])
  d[alledgex]=c2(c)
  e[alledgey]=c3(c)
  f[allcell]=c4(d[n1])
  g[alledgey]=c5(e)
  h[alledgex]=c6(f)
  i[allcell]=c7(g,h)
  j[partedgex]=c8(i[n1])
\end{lstlisting}
\caption{Example of a MSL program}
\label{fig:mslex}
\end{figure}

%----------------------------------------
\subsection{MSC Parallelization}
%----------------------------------------
The MSC parallelization is a subpart of the overall compiler MSCAC.
It makes use of the ordered list of computations $\Gamma$, which can directly be extracted from the parser, to build a parallel representation of the computations of the overall multi-stencil program.
This parallelization phase is divided in five different steps to transform $\Gamma$ to a series-parallel tree decomposition~\cite{Valdes:1979:RSP:800135.804393}.
Data parallelism is handled in the first two step, and task parallelism in the following three steps. This section gives an overview of the five steps of the MSC transformation.
The detailed formalism of these steps and the associated algorithms are described in a research report~\cite{????}.

\subsubsection*{The Ordered List of Computations: $\Gamma$}
$\Gamma$ is directly obtained from the parser. Actually, the list of computations in the MSL program is already ordered: $\Gamma$ is a direct map of this list. In the example of Figure~\ref{fig:mslex}, $\Gamma = [c_0,c_1,c_2,c_3,c_4,c_5,c_6,c_7,c_8]$.

\subsubsection*{The Synchronized Ordered List of Computations: $\Gamma_{sync}$}
In a data parallelization, data is split among processors and the same program is applied on each sub-part of the data.
This does however require additional synchronizations between processors.
For example, as a stencil kernel accesses neighbor values, values computed by another processor are needed and must be synchronized. 
The required synchronizations can be automatically computed from the ordered list of computations $\Gamma$.
A synchronization is needed each time a data read by a stencil computation has been written by a previous computation. In such a case, a computation is added before the stencil computation. This \emph{synchronization computation} reads the data to synchronize, and write the same data. The computation domain of such a synchronization computation is the mesh entity on which the data is declared. As a result $\Gamma$ is transformed to a synchronized ordered list of computations $\Gamma_{sync}$.
For example, in Figure~\ref{fig:mslex}, the stencil computation $c_1$ read the data $b$ which has been written by the computation $c_0$. For this reason the sublist $[c_0,c_1]$ of $\Gamma$ is transformed to the sublist $[c_0,sync_1,c_1]$ in $\Gamma_{sync}$. The new computation $sync_1$ read and write $b$ and is applied on the computation domain of $c_1$. As a result, a dependency is kept between $c_0$ and $sync_1$ and between $sync_1$, and $c_1$. The same transformation is performed for all stencils: $c_4$ and $c_8$. Thus, $\Gamma_{sync} = [c_0,sync_1,c_1,c_2,c_3,sync_4,c_4,c_5,c_6,c_7,sync_8,c_8]$.

\subsubsection*{The Dependency Graph: $\Gamma_{dep}$}
From the synchronized ordered list of computations $\Gamma_{sync}$, a dependency graph $\Gamma_{dep}$ is built. A dependency exists between two computations (including synchronizations) if and only if a data read has been written by a previous computation in $\Gamma_{sync}$, and if the computation domains intersect. In other words, the only case where the same data is manipulated by distinct kernels without inducing a dependency is when the computation domains do not intersect. Nodes of the dependency graph represent computations of $\Gamma_{sync}$, while edges are dependencies between them. The dependency graph $\Gamma_{dep}$ is a directed acyclic graph (\emph{DAG}). For example, the dependency DAG $\Gamma_{dep}$ of the example of Figure~\ref{fig:mslex} is presented in Figure~\ref{fig:hyb}.

\begin{figure}[t]
\begin{center}
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   \node (c0) at (0,0) {$c_0$};
   \node[right=1 of c0] (sy1) {$sync_1$};
   \node[right=1 of sy1] (c1) {$c_1$};
   \node[above right=1 of c1] (c2) {$c_2$};
   \node[below right=1 of c1] (c3) {$c_3$};
   \node[right=1 of c2] (sy4) {$sync_4$};
   \node[right=1 of c3] (c5) {$c_5$};
   \node[right=1 of sy4] (c4) {$c_4$};
   \node[right=1 of c4] (c6) {$c_6$};
   \node[below right=1 of c6] (c7) {$c_7$};
   \node[right=1 of c7] (sy8) {$sync_8$};
   \node[right=1 of sy8] (c8) {$c_8$};
 
  \path[->]
    (c0) edge node {} (sy1)
    (sy1) edge node {} (c1)
    (c1)  edge node {} (c2)
          edge node {} (c3)
    (c2) edge node {} (sy4)
    (sy4) edge node {} (c4)
    (c4) edge node {} (c6)
    (c3) edge node {} (c5)
    (c5) edge node {} (c7)
    (c6) edge node {} (c7)
    (c7) edge node {} (sy8)
    (sy8) edge node {} (c8);
\end{tikzpicture}
\caption{$\Gamma_{hybrid}$ of the example of Figure~\ref{fig:mslex}}
\label{fig:hyb}
\end{center}
\end{figure}

\subsubsection*{The Minimal Series-Parallel Graph: $\Gamma_{msp}$}
Once a dependency graph is built, many solutions can be used to build a parallel application, as for example dynamic schedulers~\cite{Augonnet2011,Gautier:2013:XRS:2510661.2511383}. In this work a static scheduling of the dependency graph is built. To do so the dependency graph is transformed to a minimal series-parallel graph~\cite{Valdes:1979:RSP:800135.804393}. As it has been shown in~\cite{Valdes:1979:RSP:800135.804393}, the transitive reduction of a DAG is a minimal series-parallel graph if and only if a forbidden shape, called \emph{N-shape} illustrated in Figure~\ref{fig:n}, is not found in the DAG. Thus, to transform $\Gamma_{dep}$ to the minimal series-parallel graph $\Gamma_{msp}$, an algorithm~\cite{Mitchell:2004:CMV:1082101.1082117} is applied to remove all \emph{N-shapes} by an over-constraint, as illustrated in Figure~\ref{fig:over}.

\begin{figure}[h!]
\begin{center}
\subfloat[][The forbidden $N$ shape.\label{fig:n}]{
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   \node[] (a) at (0,0) {$c_0$};
   \node[] (b) at (2,0) {$c_1$};
   \node[] (c) at (0,-1) {$c_2$};
   \node[] (d) at (2,-1) {$c_3$};
 
  \path[->]
    (a) edge node {} (b)
    (c) edge node {} (b)
        edge node {} (d);
  \end{tikzpicture}
  }
  \hspace{25pt}
  \subfloat[][Over-constraint on the forbidden $N$ shape.\label{fig:over}]{
  \begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   \node[] (c0) at (0,0) {$c_0$};
   \node[] (bc0) at (-1,0) {};
   \node[] (c1) at (2,0) {$c_1$};
   \node[] (c2) at (0,-1) {$c_2$};
   \node[] (bc2) at (-1,-1) {};
   \node[] (c3) at (2,-1) {$c_3$};
 
  \path[->]
    (bc0) edge [dotted] node {} (c0)
    (bc2) edge [dotted] node {} (c2)
    (c0) edge node {} (c1)
          edge [dashed] node [swap] {} (c3)
    (c2) edge node {} (c1)
        edge node {} (c3);
  \end{tikzpicture}
}
  \caption{Forbidden $N$ subgraph shape for a DAG to be minimal series-parallel.}
  \label{fig:forbidden}
\end{center}
\end{figure}

\subsubsection*{The Tree Decomposition: $\Gamma_{tsp}$}
Many works~\cite{Valdes:1979:RSP:800135.804393,Schoenmakers95anew} explains how to build a series-parallel tree decomposition from a minimal series-parallel graph. A series-parallel tree decomposition consists in the decomposition of the minimal series-parallel graph as a tree representing a set of \emph{sequences}, indicated by a $S$ node, and \emph{parallel} sections, indicated as a $P$ node. For example, the series-parallel tree decomposition $\Gamma_{tsp}$ of Figure~\ref{fig:hyb} is illustrated in Figure~\ref{fig:tsp}.

\begin{figure}[t]
\begin{center}
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   \node[] (s0) at (0,0) {$\mathcal{S}$};
   \node[] (c0) at (-3,1) {$c_0$};
   \node[] (star1) at (-2,1) {$sync_1$};
   \node[] (c1) at (-1,1) {$c_1$};

   \node[] (p0) at (0,1) {$\mathcal{P}$};
   \node[] (s1) at (-1,2) {$\mathcal{S}$};
   \node[] (p1) at (-2,3) {$\mathcal{P}$};
   \node[] (c4) at (-1,3) {$c_4$};
   \node[] (c6) at (-0,3) {$c_6$};
   \node[] (c2) at (-2.5,4) {$c_2$};
   \node[] (star4) at (-1.5,4) {$sync_4$};
   \node[] (s2) at (1,2) {$\mathcal{S}$};
   \node[] (c3) at (0.5,3) {$c_3$};
   \node[] (c5) at (1.5,3) {$c_5$};

   \node[] (c7) at (1,1) {$c_7$};
   \node[] (star8) at (2,1) {$sync_8$};
   \node[] (c8) at (3,1) {$c_8$};
 
  \path[->]
    (s0) edge node {} (c0)
         edge node {} (star1)
         edge node {} (c1)
         edge node {} (p0)
         edge node {} (c7)
         edge node {} (star8)
         edge node {} (c8)
    (p0) edge node {} (s1)
         edge node {} (s2)
    (s1) edge node {} (p1)
         edge node {} (c4)
         edge node {} (c6)
    (p1) edge node {} (c2)
         edge node {} (star4)
    (s2) edge node {} (c3)
         edge node {} (c5);
  \end{tikzpicture}
\caption{$\Gamma_{tsp}$ of Figure~\ref{fig:hyb}. Nodes $S$ represent sequences, nodes $P$ represent parallel sections.}
\label{fig:tsp}
\end{center}
\end{figure}

% \paragraph{Dump to a component assembly} Finally, from the series-parallel tree decomposition $\Gamma_{tsp}$ we could build a program using any parallel language containing \emph{parallel sections} and \emph{sequences of instructions} such as OpenMP~\cite{660313}, HPF~\cite{219857}, UPC~\cite{El-Ghazawi:2006:UUP:1188455.1188483} etc. In this paper we study the dump of $\Gamma_{tsp}$ to a component assembly which needs the introduction of \emph{control components}, described in the next section.


