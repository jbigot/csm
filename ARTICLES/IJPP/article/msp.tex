In this section we detail a static scheduling of $\Gamma_{dep}$ by using minimal series-parallel directed acyclic graphs. Such a static scheduling may not be the most efficient one, but it offers a simple fork/join task model which make possible the design of a performance model. Moreover, such a scheduling offers a simple way to propose a fusion optimization. 

%--------------------
\subsection{Series-Parallel graph}

In 1982, Valdes \& Al~\cite{Valdes:1979:RSP:800135.804393} have defined the class of Minimal Series-Parallel DAGs (MSPD). Such a graph can be decomposed as a serie-parallel tree, denoted $TSP$, where each leaf is a vertex of the MSPD DAG it represents, and whose internal nodes are labelled $S$ or $P$ to indicate respectively the series or parallel composition of sub-trees. Such a tree can be considered as a fork-join model and as a static scheduling.

Valdes \& Al~\cite{Valdes:1979:RSP:800135.804393} have identified a forbidden shape, or subgraph, called $N$, such that the following property is verified :

\begin{myth}
The transitive reduction of a DAG $G$ is MSPD if and only if it does not contain $N$ as an induced subgraph.
\end{myth}

To remove the forbidden N-shapes from the transitive reduction of $\Gamma_{dep}=(V,E)$, we have chosen to apply an over-constraint with the relation $k_0 \prec k_3$, such that a complete bipartite graph is created for the sub-dag, and can be translated to a series-parallel decomposition, as illustrated in Figure~\ref{fig:allover}.

\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   \node[] (c0) at (0,0) {$k_0$};
   \node[] (bc0) at (-1,0) {};
   \node[] (c1) at (2,0) {$k_1$};
   \node[] (c2) at (0,-1) {$k_2$};
   \node[] (bc2) at (-1,-1) {};
   \node[] (c3) at (2,-1) {$k_3$};
 
  \path[->]
    (bc0) edge [dotted] node {} (c0)
    (bc2) edge [dotted] node {} (c2)
    (c0) edge node {} (c1)
          edge [dashed] node [swap] {} (c3)
    (c2) edge node {} (c1)
        edge node {} (c3);
  \end{tikzpicture}
\caption{Over-constraint on the forbidden $N$ shape.}
\label{fig:over}
\end{center}
\end{figure}

After these over-constraints are applied, $\Gamma_{dep}$ is MSPD. Valdes \& Al~\cite{Valdes:1979:RSP:800135.804393} have proposed a linear algorithm to know if a DAG is MSPD and, if it is, to decompose it to its associated binary decomposition tree. As a result, the binary tree decomposition algorithm of Valdes \& Al can be applied on $\Gamma_{dep}$ to get the $TSP$ static scheduling of the multi-stencil program.

\paragraph{\textbf{Example}} The Serie-Parallel tree decomposition of the example given in Figure~\ref{fig:exmsl}, which is built from the dependency graph of Figure~\ref{fig:depdep} is given in Figure~\ref{fig:tree}.

\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   %\node[circle,draw=black,fill=black,scale=0.3] (c0s) at (0,0) {};
   %\node[circle,draw=black,fill=black,scale=0.3] (c8d) at (9,0) {};

   \node[] (s8) at (4.5,0.5) {$\mathcal{S}$};
   \node[] (s9) at (3,1.5) {$\mathcal{S}$};

   %reduction c0 c1
   \node[] (s0) at (1.5,2.5) {$\mathcal{S}$};
   \node[] (s1) at (1,3.5) {$\mathcal{S}$};
   \node[] (c0) at (0.5,4.5) {$k_0$};
   \node[] (star1) at (1.5,4.5) {$k_{0;1}^{sync}$};
   \node[] (c1) at (2,3.5) {$k_1$};
   
   %reduction c7 c8
   \node[] (s2) at (7.5,1.5) {$\mathcal{S}$};
   \node[] (s3) at (7,2.5) {$\mathcal{S}$};
   \node[] (c7) at (6.5,3.5) {$k_7$};
   \node[] (star8) at (7.5,3.5) {$k_{7;8}^{sync}$};
   \node[] (c8) at (8,2.5) {$k_8$};

   \node[] (p1) at (4.5,2.5) {$\mathcal{P}$};
   %reduction c3 c5
   \node[] (s5) at (5.5,3.5) {$\mathcal{S}$};
   \node[] (c3) at (5,4.5) {$k_3$};
   \node[] (c5) at (6,4.5) {$k_0$};
   %reduction c2 c6
   \node[] (s6) at (3.5,3.5) {$\mathcal{S}$};
   \node[] (p0) at (2.8,4.5) {$\mathcal{P}$};
   \node[] (c2) at (2.3,5.5) {$k_2$};
   \node[] (star4) at (3.3,5.5) {$k_{1;4}^{sync}$};
   \node[] (s7) at (4.2,4.5) {$\mathcal{S}$};
   \node[] (c4) at (3.8,5.5) {$k_4$};
   \node[] (c6) at (4.8,5.5) {$k_6$};
 
  \path[->]
    %(c0s) edge node {} (c8d)

    (s8) edge node {} (s9)
         edge node {} (s2)
    (s9) edge node {} (p1)
         edge node {} (s0)

    %reduction c0 c1
    (s0) edge node {} (s1)
         edge node {} (c1)
    (s1) edge node {} (c0)
         edge node {} (star1)
    %reduction c7 c8
    (s2) edge node {} (s3)
         edge node {} (c8)
    (s3) edge node {} (c7)
        edge node {} (star8)

    (p1) edge node {} (s6)
         edge node {} (s5)
    %reduction c3 c5
    (s5) edge node {} (c3)
         edge node {} (c5)
    %reduction c2 c6
    (s6) edge node {} (p0)
         edge node {} (s7)
    %reduction c2 *4
    (p0) edge node {} (c2)
         edge node {} (star4)
    (s7) edge node {} (c4)
         edge node {} (c6);
  \end{tikzpicture}
\caption{Serie-Parallel tree decomposition of the example of program of Figure~\ref{fig:exmsl}}
\label{fig:tree}
\end{center}
\end{figure}

%--------------------
\subsection{Performance model}

%--------------------
\subsection{Fusion optimization}

Using MSL, it is possible to ask for data parallelization of the application, or for an hybrid parallelization. Even though the MSL language is not dedicated to produce very optimized stencil codes, but to produces the parallel pattern of the application, building the $TSP$ tree make available an easy optimization when the data parallelization technique is the only one used. This optimization consists in proposing a valid merge of some computation kernels inside a single space loop. As a result, the user can use this valid fusion of kernels or not when implementing those. 

Those fusions can be computed from the canonical form of the $TSP$ tree decomposition. The canonical form consists in recursively merging successive $S$ vertices or successive $P$ vertices of $TSP$.

The fusion function $F_{fus}$ is described in Algorithm~\ref{alg:fus}, where $parent(k)$ returns the parent vertex of $k$ in the tree, and where $k_{i;j}^{fus}$ represents the fusion of $k_i$ and $k_j$ keeping the sequential order $i;j$ if $i$ is computed before $j$ in $TSP$. Finally, $type(k)$ returns $comp$ if the kernel is a computation kernel, and $sync$ otherwise.

\begin{algorithm}
\caption{$F_{fus}$}
\label{alg:fus}
\begin{algorithmic}[1]
\Procedure{$F_{fus}$} {$TSP(V,E)$}
\For {$(k_i,k_j) \in V^2$}
\If {parent($k_i$)==parent($k_j$)}
\If {$type(k_i)=type(k_j)=comp$}
\If {parent($k_i$)==$S$}
\If {$D_i==D_j$}
\State propose the fusion $k_{i;j}^{fus}$
\EndIf
\ElsIf {parent($k_i$)==$P$}
\If {$D_i==D_j$ and $R_i \cap R_j \neq \emptyset$}
\State propose the fusion $k_{i;j}^{fus}$
\EndIf
\EndIf
\EndIf
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

We are not arguing that such a simple fusion algorithm could be as good as complex cache optimization techniques which can be found in stencil DSLs for example~\cite{spaaTangCKLL11}. However, this fusion takes place at a different level and can bring performance improvments as it will be illustrated in Section~\ref{sect:eval}. This fusion algorithm relies on very simple statements:
\begin{itemize}
\item Two successive computation kernels $k_i$ and $k_j$ which are under the same parent vertex $S$ in TSP are, by construction, data dependant. As a result, what is written by the first one is read by the second one. Thus, at least one data is common to those computations (the one written by $k_i$). Thus, if the computation domains verify $D_i=D_j$, the fusion of $k_i$ and $k_j$ will decrease cache misses.
\item Two successive computation kernels $k_i$ and $k_j$ which are under the same parent vertex $P$ in TSP are not, by construction, data dependant. However, if the computation domains verify $D_i=D_j$, and if $R_i \cap R_j \neq \emptyset$ cache misses could also be decreased by the fusion $k_{i;j}^{fus}$.
\end{itemize}






