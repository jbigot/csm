In this section we detail a static scheduling of $\Gamma_{dep}$ by using minimal series-parallel directed acyclic graphs. Such a static scheduling may not be the most efficient one, but it offers a simple fork/join task model which make possible the design of a performance model. Moreover, such a scheduling offers a simple way to propose a fusion optimization. 

%--------------------
\subsection{Series-Parallel graph}
\label{sect:tsp}

In 1982, Valdes \& Al~\cite{Valdes:1979:RSP:800135.804393} have defined the class of Minimal Series-Parallel DAGs (MSPD). Such a graph can be decomposed as a serie-parallel tree, denoted $TSP$, where each leaf is a vertex of the MSPD DAG it represents, and whose internal nodes are labelled $S$ or $P$ to indicate respectively the series or parallel composition of sub-trees. Such a tree can be considered as a fork-join model and as a static scheduling.

Valdes \& Al~\cite{Valdes:1979:RSP:800135.804393} have identified a forbidden shape, or subgraph, called $N$, such that the following property is verified :

\begin{myth}
The transitive reduction of a DAG $G$ is MSPD if and only if it does not contain $N$ as an induced subgraph.
\end{myth}

To remove the forbidden N-shapes from the transitive reduction of $\Gamma_{dep}=(V,E)$, we have chosen to apply an over-constraint with the relation $k_0 \prec k_3$, such that a complete bipartite graph is created for the sub-dag, and can be translated to a series-parallel decomposition, as illustrated in Figure~\ref{fig:allover}.

\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   \node[] (c0) at (0,0) {$k_0$};
   \node[] (bc0) at (-1,0) {};
   \node[] (c1) at (2,0) {$k_1$};
   \node[] (c2) at (0,-1) {$k_2$};
   \node[] (bc2) at (-1,-1) {};
   \node[] (c3) at (2,-1) {$k_3$};
 
  \path[->]
    (bc0) edge [dotted] node {} (c0)
    (bc2) edge [dotted] node {} (c2)
    (c0) edge node {} (c1)
          edge [dashed] node [swap] {} (c3)
    (c2) edge node {} (c1)
        edge node {} (c3);
  \end{tikzpicture}
\caption{Over-constraint on the forbidden $N$ shape.}
\label{fig:over}
\end{center}
\end{figure}

After these over-constraints are applied, $\Gamma_{dep}$ is MSPD. Valdes \& Al~\cite{Valdes:1979:RSP:800135.804393} have proposed a linear algorithm to know if a DAG is MSPD and, if it is, to decompose it to its associated binary decomposition tree. As a result, the binary tree decomposition algorithm of Valdes \& Al can be applied on $\Gamma_{dep}$ to get the $TSP$ static scheduling of the multi-stencil program.

\paragraph{\textbf{Example}} The Serie-Parallel tree decomposition of the example given in Figure~\ref{fig:exmsl}, which is built from the dependency graph of Figure~\ref{fig:depdep} is given in Figure~\ref{fig:tree}.

\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   %\node[circle,draw=black,fill=black,scale=0.3] (c0s) at (0,0) {};
   %\node[circle,draw=black,fill=black,scale=0.3] (c8d) at (9,0) {};

   \node[] (s8) at (4.5,0.5) {$\mathcal{S}$};
   \node[] (s9) at (3,1.5) {$\mathcal{S}$};

   %reduction c0 c1
   \node[] (s0) at (1.5,2.5) {$\mathcal{S}$};
   \node[] (s1) at (1,3.5) {$\mathcal{S}$};
   \node[] (c0) at (0.5,4.5) {$k_0$};
   \node[] (star1) at (1.5,4.5) {$k_{0;1}^{sync}$};
   \node[] (c1) at (2,3.5) {$k_1$};
   
   %reduction c7 c8
   \node[] (s2) at (7.5,1.5) {$\mathcal{S}$};
   \node[] (s3) at (7,2.5) {$\mathcal{S}$};
   \node[] (c7) at (6.5,3.5) {$k_7$};
   \node[] (star8) at (7.5,3.5) {$k_{7;8}^{sync}$};
   \node[] (c8) at (8,2.5) {$k_8$};

   \node[] (p1) at (4.5,2.5) {$\mathcal{P}$};
   %reduction c3 c5
   \node[] (s5) at (5.5,3.5) {$\mathcal{S}$};
   \node[] (c3) at (5,4.5) {$k_3$};
   \node[] (c5) at (6,4.5) {$k_0$};
   %reduction c2 c6
   \node[] (s6) at (3.5,3.5) {$\mathcal{S}$};
   \node[] (p0) at (2.8,4.5) {$\mathcal{P}$};
   \node[] (c2) at (2.3,5.5) {$k_2$};
   \node[] (star4) at (3.3,5.5) {$k_{1;4}^{sync}$};
   \node[] (s7) at (4.2,4.5) {$\mathcal{S}$};
   \node[] (c4) at (3.8,5.5) {$k_4$};
   \node[] (c6) at (4.8,5.5) {$k_6$};
 
  \path[->]
    %(c0s) edge node {} (c8d)

    (s8) edge node {} (s9)
         edge node {} (s2)
    (s9) edge node {} (p1)
         edge node {} (s0)

    %reduction c0 c1
    (s0) edge node {} (s1)
         edge node {} (c1)
    (s1) edge node {} (c0)
         edge node {} (star1)
    %reduction c7 c8
    (s2) edge node {} (s3)
         edge node {} (c8)
    (s3) edge node {} (c7)
        edge node {} (star8)

    (p1) edge node {} (s6)
         edge node {} (s5)
    %reduction c3 c5
    (s5) edge node {} (c3)
         edge node {} (c5)
    %reduction c2 c6
    (s6) edge node {} (p0)
         edge node {} (s7)
    %reduction c2 *4
    (p0) edge node {} (c2)
         edge node {} (star4)
    (s7) edge node {} (c4)
         edge node {} (c6);
  \end{tikzpicture}
\caption{Serie-Parallel tree decomposition of the example of program of Figure~\ref{fig:exmsl}}
\label{fig:tree}
\end{center}
\end{figure}

%--------------------
\subsection{Performance model}

In this subsection are introduced two performance model, one for the data parallelization technique, and one for the hybrid data and task parallelization technique, both previously explained.

The performance model of a data parallelization technique is inspired from the Bulk Synchronous Parallel model. Actually data parallelization technique consider that each process has its own part of the domain. Thus the performance model reveals that the computation time is the sum of the reference sequential time divided by the number of processes, and of the time spent in communications between processes. Thus, for
\begin{itemize}
\item $T_{SEQ}$ the sequential reference time, 
\item $P$ the total number of processes, 
\item $T_{COM}$ the communications time, 
\end{itemize}
the total computation time is
\begin{equation}
T = \frac{T_{SEQ}}{P} + T_{COM}.
\end{equation}

% The time spent into communications between MPI processes can be represented as the time spent to send data onto the network and the time spent to synchronize processes. Thus, for
% \begin{itemize}
% \item $S$ the total number of bytes sent during the simulation
% \item $T_s$ the time to send one byte
% \item $N_{COM}$ the number of communication steps
% \item $Sync$ the time to synchronize two MPI processes for communications
% \end{itemize}
% the communication time is
% \begin{equation*}
% T_{COM} = S \times T_s + N_{COM} \times Sync.
% \end{equation*}

Thus, when the number of processes $P$ increase in data parallelization, the performance model limit is $T_{COM}$
\begin{equation}
\lim\limits_{P \rightarrow +\infty} T = T_{COM}.
\end{equation}

As a result, the critical point for performance is when $T_{COM} \geq \frac{T_{SEQ}}{P}$, which happens naturally in data parallelization as $T_{COM}$ will increase with the number of processes, and $\frac{T_{SEQ}}{P}$ decrease with the number of processes.

This limitation is always true, but can be delayed by different strategies. First, it is possible to perform communications through the network while computations are performed simultaneously. Second, it is possible to introduce another kind of parallelization, task parallelization. Thus, for the same total number of processes, only a part of them are used for data parallelization, and the rest are used for task parallelism. As a result, $\frac{T_{SEQ}}{P}$ will continue to decrease but $T_{COM}$ will increase later. This second strategy is the one studied in the following hybrid performance model.

For an hybrid (data and task) parallelization technique, and for
\begin{itemize}
\item $P_{data}$ the number of processes used for data parallelization,
\item $P_{task}$ the number of processes used for task parallelization,
\item such that $P = P_{data} \times P_{task}$ is the total number of processes used,
\item $T_{task}$ the overhead time due to task parallelization technique,
\item and $F_{task}$ the task parallelization degree of the application,
\end{itemize}
the total computation time is
\begin{equation}
T = \frac{T_{SEQ}}{P_{data} \times F_{task}} + T_{COM} + T_{task}
\end{equation}

The time overhead due to task parallelization can be represented as the time spent to create a pool of threads and the time spent to synchronize those threads. Thus, for
\begin{itemize}
\item $T_{cr}$ the total time to create the pool of threads (may happened more than once), 
\item $T_{sync}$ the total time spent to synchronize threads, 
\end{itemize}
the overhead is
\begin{equation*}
T_{task} = T_{cr} + T_{sync}.
\end{equation*}

The task parallelization degree of the application $F_{task}$ is the limitation of a task parallelization technique. As explained before, a task parallelization technique is based on the dependency graph of the application. Thus, this dependency graph must expose enough parallelism for the number of available threads. For this performance model we consider that 
\begin{equation*}
F_{task} = P_{task}.
\end{equation*}
However, the upper bound of $F_{task}$ is constrained by the dependency graph of the application and by the time spent in each task.

\medskip
As a result when $P_{data}$ is small a data parallelization technique may be more efficient, while an hybrid parallelization could be interesting at some point to improve performance. The question asked here is when is it interesting to use hybrid parallelization.

To answer this question lets consider the two parallelization techniques, data only and hybrid. We denote
\begin{itemize}
\item $P_{data1}$ the total number of processes entirely used by the data only parallelization,
\item $P_{data2}$ the number of processes used for data parallelization in the hybrid parallelization,
\item and $P_{task}$ the number of processes used for task parallelization in the hybrid parallelization,
\item such that $P_{data1} = P_{data2} \times P_{task}$.
\end{itemize}

We search the point where the data parallelization is less efficient than the hybrid parallelization. Thus, 
\begin{equation*}
\frac{T_{SEQ}}{P_{data1}} + T_{COM1} \geq \frac{T_{SEQ}}{P_{data2} \times P_{task}} + T_{COM2} + T_{task}.
\end{equation*}

This happens when
\begin{equation}
T_{COM1} \geq T_{COM2} + T_{task}
\end{equation}

This performance model will be validated and will help to explain results the Section~\ref{sect:eval}.

%--------------------
\subsection{Fusion optimization}

Using MSL, it is possible to ask for data parallelization of the application, or for an hybrid parallelization. Even though the MSL language is not dedicated to produce very optimized stencil codes, but to produces the parallel pattern of the application, building the $TSP$ tree make available an easy optimization when the data parallelization technique is the only one used. This optimization consists in proposing a valid merge of some computation kernels inside a single space loop. As a result, the user can use this valid fusion of kernels or not when implementing those. 

Those fusions can be computed from the canonical form of the $TSP$ tree decomposition. The canonical form consists in recursively merging successive $S$ vertices or successive $P$ vertices of $TSP$.

The fusion function $F_{fus}$ is described in Algorithm~\ref{alg:fus}, where $parent(k)$ returns the parent vertex of $k$ in the tree, and where $k_{i;j}^{fus}$ represents the fusion of $k_i$ and $k_j$ keeping the sequential order $i;j$ if $i$ is computed before $j$ in $TSP$. Finally, $type(k)$ returns $comp$ if the kernel is a computation kernel, and $sync$ otherwise.

\begin{algorithm}
\caption{$F_{fus}$}
\label{alg:fus}
\begin{algorithmic}[1]
\Procedure{$F_{fus}$} {$TSP(V,E)$}
\For {$(k_i,k_j) \in V^2$}
\If {parent($k_i$)==parent($k_j$)}
\If {$type(k_i)=type(k_j)=comp$}
\If {parent($k_i$)==$S$}
\If {$D_i==D_j$}
\State propose the fusion $k_{i;j}^{fus}$
\EndIf
\ElsIf {parent($k_i$)==$P$}
\If {$D_i==D_j$ and $R_i \cap R_j \neq \emptyset$}
\State propose the fusion $k_{i;j}^{fus}$
\EndIf
\EndIf
\EndIf
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

We are not arguing that such a simple fusion algorithm could be as good as complex cache optimization techniques which can be found in stencil DSLs for example~\cite{spaaTangCKLL11}. However, this fusion takes place at a different level and can bring performance improvments as it will be illustrated in Section~\ref{sect:eval}. This fusion algorithm relies on very simple statements:
\begin{itemize}
\item Two successive computation kernels $k_i$ and $k_j$ which are under the same parent vertex $S$ in TSP are, by construction, data dependant. As a result, what is written by the first one is read by the second one. Thus, at least one data is common to those computations (the one written by $k_i$). Thus, if the computation domains verify $D_i=D_j$, the fusion of $k_i$ and $k_j$ will decrease cache misses.
\item Two successive computation kernels $k_i$ and $k_j$ which are under the same parent vertex $P$ in TSP are not, by construction, data dependant. However, if the computation domains verify $D_i=D_j$, and if $R_i \cap R_j \neq \emptyset$ cache misses could also be decreased by the fusion $k_{i;j}^{fus}$.
\end{itemize}






