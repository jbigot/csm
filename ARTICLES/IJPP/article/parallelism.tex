% Multi-stencil mesh-based numerical simulation can be parallelized in various ways and is an interesting kind of application to take advantage of modern heterogeneous HPC architectures, mixing clusters, multi-cores CPUs, vectorization units, GPGPU and many-core accelerators.

%------------------------------
\subsection{Data parallelism}
\label{sect:dataparal}
In a data parallelization technique, the idea is to split the data on which the program is computed into balanced sub-parts, one for each available resource. The same sequential program can afterwards be applied on each sub-part simultaneously, with some additioinal synchronizations between resources to update the data not computed locally, and thus to guarantee a correct result.

More formally, the data parallelization of a multi-stencil program $\mathcal{MSP}(T,\mathcal{M},\mathcal{E},\mathcal{D},\Delta, \mathcal{S},\Gamma)$ consists in, first, a partitioning of the mesh $\mathcal{M}$ in $p$ balanced sub-meshes (for $p$ resources) $\mathcal{M}=\{\mathcal{M}_0,\dots,\mathcal{M}_{p-1}\}$. This step can be performed by an external graph partitionner~\cite{} and is not adressed by this paper. As entities and as data are mapped onto the mesh, the set of entities and the set of data $\Delta$ are partitionned the same way than the mesh: $\mathcal{E}=\{\mathcal{E},\dots,\mathcal{E}{p-1}\}$, $\Delta=\{\Delta_0,\dots,\Delta_{p-1}\}$. The second step of the parallelization is to identify in $\Gamma$ the needed synchronizations between resources to update data, and thus to build a new ordered list of computations $\Gamma_{sync}$.

\begin{mydef}
For $n$ the number of computations in $\Gamma$, and for $i,j$ such that $i<j<n$, a \textit{synchronisation} is needed between $k_i$ and $k_j$, denoted $k_i \pprec k_j$, if $\exists (r_j,n_j) \in R_j$ such that $w_i=r_j$ and $n_j\neq identity$ ($k_j$ is a stencil computation). Moreover, the data to update is $\{w_i\}$.
\end{mydef}

Actually, a synchronization is needed by the data read by a stencil computation (not local), and only if this data has been modified before, which means that it has been written before. This synchronization is needed because a neighborhood function $n \in \mathcal{N}$ of a stencil computation involves values computed on different resources.

\begin{mydef}
A synchronization between two computations $k_i \pprec k_j$ is defined as a specific computation 
\begin{equation*}
k_{sync}(S,R,w,comp)
\end{equation*}
\end{mydef}
where $S=\emptyset$, $R=\{(w_i,n_j)\}$, $w=(w_i,n_j)$ and $comp=identity$. In other words, $w_i$ has to be synchronized for the neighborhood $n_j$.

\begin{mydef}
If $k_i \pprec k_j$, $k_j$ is replaced by the list
\begin{equation*}
[k_{sync}(\emptyset,\{(w_i,n_j)\},(w_i,n_j),identity), k_j]
\end{equation*}
\end{mydef}

When a data parallelism is applied, the other type of computation which is responsible for additional synchronizations is the reduction. Actually, the reduction is applied locally on each subset of entities, on each resource, which results in $p$ (number of resources) scalar value, one on each resource. Thus, to perform the final reduction a synchronization is needed to get the $p$ scalars onto a single resource, to finally get the reduction scalar.

\begin{mydef}
For a reduction kernel $k(S,R,(w,D),comp)$, if we denote by $w^i$, $0 \leq i<p$, the local $w$ computed on the resource $i$, a reduction synchronization is defined as the specific computation 
\begin{equation*}
k_{sync}(S,R_s,w,comp)
\end{equation*}
\end{mydef}
where, $S=\emptyset$, $R=\{(w^0,entity(w^0)) \dots (w^{p-1},entity(w^{p-1}))\}$ and $w$ and $comp$ are the same than the actual reduction.

One can notice that both types of synchronizations are performed by all resources.

\begin{mydef}
The concatenation of two ordered lists of respectively $n$ and $m$ computations $l_1=[k_i]_{0 \leq i \leq n-1}$ and $l_2=[c'_i]_{0 \leq i \leq m-1}$ is denoted $l_1 \cdot l_2$ and is equal to a new ordered list $l_3=[c_0,\dots,c_{n-1},c'_0,\dots,c'_{m-1}]$.
\end{mydef}

\begin{mydef}
From the ordered list of computation $\Gamma$, a new ordered list $\Gamma_{sync}$ is obtained from the call $\Gamma_{sync} = F_{sync}(\Gamma,0)$, where $F_{sync}$ is the recursive function defined as
\begin{equation*}
F_{sync}(\Gamma,j) = 
\begin{cases} 	[] \mbox{ if }j=|\Gamma|\\
				[k_{sync}(\emptyset,\{w_i\},w_i,identity,\emptyset), k_j] \cdot F_{sync}(\Gamma,j+1)\\
				\text{ } \qquad \forall i<j \text{, and }k_i \pprec k_j\\
				[k_j, k_{sync}(\emptyset,,w_j,comp_j)] \cdot F_{sync}(\Gamma,j+1)\\ 
				\text{ } \qquad \mbox{if } w_j \in \mathcal{S} \text{, and } R_j=\{(r_j,entity(r_j))\}\\
				[\Gamma[j]] \cdot F_{sync}(\Gamma,j+1) \text{ else}.
\end{cases}
\end{equation*}
\end{mydef}


 The final step of this parallelization is to run $\Gamma_{sync}$ on each resource. Thus, for each resource $0 \leq m \leq p-1$ a multi-stencil program, defined by
\begin{equation}
\mathcal{MSP}_m(T,\mathcal{M}_m,\mathcal{E}_m,\mathcal{D}_m,\Delta_m,\mathcal{S},\Gamma_{sync}),
\end{equation}
is performed.

%------------------------------
\subsection{Hybrid parallelism}
A task parallelization technique is a technique to transform a program as a dependency graph of different tasks. A dependency graph exhibits parallel tasks, or on the contrary sequential execution of tasks. Such a dependency graph can directly be given to a dynamic scheduler, or can be statically scheduled. In this paper, we introduce task parallelism by building the dependency graph between computations of the sequential list $\Gamma_{sync}$ (which itself take into account data parallelism).

\begin{mydef}
For two computations $k_i(S_i,R_i,(w_i,D_i),comp_i)$ and $k_j(S_j,R_j,(w_j,D_j),comp_j)$, with $i < j$, it is said that $k_j$ is data dependant from $k_i$, denoted $k_i\prec k_j$, if $w_i \in R_j$. In this case, $k_i$ has to be computed before $k_j$. The binary relation $k_i\prec k_j$ represents a \textit{dependency}.
\end{mydef}

\begin{mydef}
A directed acyclic graph (DAG) $G(V,A)$ is a graph where the edges are directed from a source to a destination vertex and where, following the direction of edges, no cycle can be found from a vertex $u$ to itself. A directed edge is called an arc, and for two vertices $v,u \in V$ an arc from $u$ to $v$ is denoted $(\overset{\frown}{u,v}) \in A$.
\end{mydef}

From an ordered list of computations $\Gamma_{sync}$, a directed dependency graph $\Gamma_{dep}(V,A)$ can be built finding all pairs of computations $k_i(S_i,R_i,(w_i,D_i),comp_i)$ and $k_j(S_j,R_j,(w_j,D_j),comp_j)$, with $i<j$, such that $k_i \prec k_j$. 

\begin{mydef}
For two directed graphs $G(V,A)$ and $G'(V',A')$, the union $(V,A)\cup (V',A')$ is defined as the union of each set $(V\cup V', A \cup A')$.
\end{mydef}

\begin{mydef}
From the ordered list $\Gamma_{sync}$ of computation kernels, a directed dependency graph $\Gamma_{dep}(V,A)$ is obtained from the call $T_{dep}(\Gamma_{sync},0)$, where $F_{dep}$ is the recursive function
\begin{equation*}
F_{dep}(\Gamma_{sync},j) = 
\begin{cases} 	(\{\},\{\}) \mbox{ if }j=|\Gamma_{sync}|\\
				(k_j, \{(\overset{\frown}{k_i,k_j})\mbox{, }\forall i < j \mbox{, } k_i\prec k_j \})\\
				\text{ } \qquad \cup F_{dep}(\Gamma_{sync},j+1) \mbox{ if }j<|\Gamma_{sync}|
\end{cases}
\end{equation*}
\end{mydef}

This constructive function is possible because the input is an ordered list. Actually, if $k_i\prec k_j$ then $i<j$. As a result, $k_i$ is already in $V$ when the arc $(\overset{\frown}{k_i,k_j})$ is built. 

\begin{myprop}
The directed graph $\Gamma_{dep}$ is an acyclic graph.
\end{myprop}

\begin{proof}
$\Gamma_{dep}$ is built from $\Gamma_{sync}$ which is an ordered and sequential list of computations. Moreover, each computation of the list $\Gamma_{sync}$ is associated to a vertex of $V$, even if the same computation is represented more than once in $\Gamma_{sync}$. As a result it is not possible to go back to a previous computation and to create a cycle.
\end{proof}

As a result of the hybrid parallelization, each resource $0 \leq m \leq p-1$ perform a multi-stencil program, defined by
\begin{equation*}
\mathcal{MSP}_m(T,\mathcal{M}_m,\mathcal{E}_m,\mathcal{D}_m,\Delta_m,\Gamma_{dep}).
\end{equation*}
The set of computations $\Gamma_{dep}$ is a dependency graph between computations $k_i$ of $\Gamma$ and the synchronization kernels added into $\Gamma_{sync}$. $\Gamma_{dep}$ can be built from the call to 
\begin{equation*}
F_{dep}(F_{sync}(\Gamma,0),0).
\end{equation*}

