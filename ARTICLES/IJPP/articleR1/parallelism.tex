In a computation $k(S,R,(w,D),comp)$, the $comp$ part is provided by the developer after the MSC compilation phase.
This part does therefore not have any impact on compilation concerns.
Thus, to simplify notations in the rest of this paper, we use the shortcut notation $k(S,R,(w,D))$ instead of $k(S,R,(w,D),comp)$.

%------------------------------
\subsection{Data parallelism}
\label{sect:dataparal}
In a data parallelization technique, the idea is to split data, or quantities, on which the program is computed into
%JB: oui, certes... mais ca n'apporte rien comme info ici.
% balanced 
sub-domains, one for each execution resource.
The same program is applied to each sub-domain simultaneously with some additional synchronizations to ensure coherence.

\medskip
More formally, the data parallelization of a multi-stencil program 
\begin{equation*}
\mathcal{MSP}(\mathcal{M},\Phi,\mathcal{D},\mathcal{N},\Delta, \mathcal{S},T,\Gamma)
\end{equation*}
consists in a partitioning of the mesh $\mathcal{M}$ in $p$ sub-meshes $\mathcal{M}=\{\mathcal{M}_0,\dots,\mathcal{M}_{p-1}\}$.
This step can be performed by an external graph partitioner~\cite{Pellegrini:1996:SSP:645560.658570,DBLP:conf/ieeehpcs/HeleneS13,lachat:hal-00768916} and is addressed by the third party DDS component instantiated from GA. 

As entities and quantities are mapped on the mesh, the set of groups of mesh entities and the set of quantities $\Delta$ are partitioned the same way as the mesh: $\Phi=\{\Phi_0,\dots,\Phi_{p-1}\}$, $\Delta=\{\Delta_0,\dots,\Delta_{p-1}\}$. 

The second step of the parallelization is to identify in $\Gamma$ the synchronizations required to update data.
It leads to the construction of a new ordered list of computations $\Gamma_{sync}$.

\begin{mydef}
For $n$ the number of computations in $\Gamma$, and for $i,j$ such that $i<j<n$, a \textit{synchronization} is needed between $k_i$ and $k_j$, denoted $k_i \pprec k_j$, if $\exists (r_j,n_j) \in R_j$ such that $w_i=r_j$ and $n_j\neq identity$ ($k_j$ is a stencil computation). The quantity to synchronize is $\{w_i\}$.
\label{def:sync}
\end{mydef}

A synchronization is needed for the quantity read by a stencil computation (not local), if this quantity has been written before. This synchronization is needed because a neighborhood function $n \in \mathcal{N}$ of a stencil computation involves values computed on different resources.

However, as a multi-stencil program is an iterative program, computations which happen after $k_j$ at the time iteration $t$ have also been computed before $k_j$ at the previous time iteration $t-1$. For this reason another case of synchronization has to be defined.

\begin{mydef}
For $n$ the number of computations in $\Gamma$ and $j<n$, if $\exists (r_j,n_j) \in R_j$ such that $n_j\neq identity$ and such that for all $i<j$, $k_i \not \pprec k_j$, a \textit{synchronization} is needed between $k_l^{t-1}$ and $k_j^t$, where $j<l<n$, denoted $k_l^{t-1} \pprec k_j^{t}$, if $w_l=r_j$. The quantity to synchronize is $\{w_l\}$.
\label{def:sync2}
\end{mydef}

\begin{mydef}
A synchronization between two computations $k_i \pprec k_j$ is defined as a specific computation 
\begin{equation*}
k_{i,j}^{sync}(S,R,(w,D)), 
\end{equation*}
where $S=\emptyset$, $R=\{(r,n)\}=\{(w_i,n_j \in \mathcal{N}\}$, $(w,D)=(w_i,\bigcup_{\phi \in D_j} n_j(\phi)))$. In other words, $w_i$ has to be synchronized for the neighborhood $n_j$ for all entities of $D_j$.
\label{def:sync3}
\end{mydef}

\begin{mydef}
If $k_i \pprec k_j$, $k_j$ is replaced by the list
\begin{equation*}
[k_{i,j}^{sync}, k_j]
\end{equation*}
where the synchronization operation has been added.
\end{mydef}

When data parallelism is applied, the other type of computation which is responsible for additional synchronizations is the reduction. Actually, the reduction is first applied locally on each subset of entities, on each resource. Thus, $p$ (number of resources) scalar values are obtained. For this reason, to perform the final reduction, a set of synchronizations are needed to get the final reduced scalar. As most parallelism libraries (MPI, OpenMP) already propose a reduction synchronization with their own optimizations, we simply replace the reduction computation by itself annotated by $red$.

\begin{mydef}
A reduction kernel $k_j(S_j,R_j,(w_j,D_j))$, where $w$ is a scalar, is replaced by $k^{red}_j(S_j,R_j,(w_j,D_j))$.
\label{def:red}
\end{mydef}

One can notice that both types of synchronizations are performed by all resources.

\begin{mydef}
The concatenation of two ordered lists of respectively $n$ and $m$ computations $l_1=[k_i]_{0 \leq i \leq n-1}$ and $l_2=[k'_i]_{0 \leq i \leq m-1}$ is denoted $l_1 \cdot l_2$ and is equal to a new ordered list $l_3=[k_0,\dots,k_{n-1},k'_0,\dots,k'_{m-1}]$.
\end{mydef}

\begin{mydef}
From the ordered list of computation $\Gamma$, a new synchronized ordered list $\Gamma_{sync}$ is obtained from the call $\Gamma_{sync} = F_{sync}(\Gamma,0)$, where $F_{sync}$ is the recursive function defined in Algorithm~\ref{alg:sync}.
\end{mydef}

Algorithm~\ref{alg:sync} follows previous definitions to build a new ordered list which includes synchronizations. In this algorithm, lines 7 to 19 apply Definition~(\ref{def:sync}), lines 20 to 29 apply Definition~(\ref{def:sync2}), and finally lines 34 and 35 apply Definition~(\ref{def:red}). Finally, line 37 of the algorithm is the recursive call.

\begin{algorithm}
\caption{$F_{sync}$ recursive function}
\label{alg:sync}
\begin{algorithmic}[1]
\Procedure{$F_{sync}$} {$\Gamma$,$j$}
\State $k_j = \Gamma[j]$
\State $list = []$
\If {$j=|\Gamma|$}
\State return $list$
\ElsIf {$\exists (r_j,n_j) \in R_j$ such that $n_j\neq identity$}
\For {all $(r_j,n_j) \in R_j$ such that $n_j\neq identity$}
\State found = false
\For {$0 \leq i<j$}
\State $k_i = \Gamma[i]$
\If {$k_i \pprec k_j$}
\State found = true
\State $S = \emptyset$
\State $R = \{(w_i,n_j)\}$
\State $(w,D) = (w_i,\bigcup_{\phi \in D_j} n_j(\phi)))$
%\State $comp = identity$
\State $list.[k_{i;j}^{sync}(S,R,(w,D))]$%,comp)]$
\EndIf
\EndFor
\If {!found}
\For {$j<i\leq n$}
\State $k_i = \Gamma[i]$
\If {$k_i \pprec k_j$}
\State $S = \emptyset$
\State $R = \{(w_i,n_j)\}$
\State $(w,D) = (w_i,\bigcup_{\phi \in D_j} n_j(\phi)))$
%\State $comp = identity$
\State $list.[k_{i;j}^{sync}(S,R,(w,D))]$%,comp)]$
\EndIf
\EndFor
\EndIf
\State $list \cdot [k_j]$
\EndFor
\ElsIf {$w_j \in \mathcal{S}$}
\State $list.[k^{red}_j]$
\Else
\State $list.[k_j]$
\EndIf
\State return $list \cdot F_{sync}(\Gamma,j+1)$
\EndProcedure
\end{algorithmic}
\end{algorithm}


 The final step of this parallelization is to run $\Gamma_{sync}$ on each resource. Thus, for each resource $0 \leq r \leq p-1$ the multi-stencil program 
\begin{equation}
\mathcal{MSP}_r(\mathcal{M}_r,\Phi_r,\mathcal{D}_r,\mathcal{N},\Delta_r,\mathcal{S},T,\Gamma_{sync}),
\end{equation}
is performed.

\paragraph{\textbf{Example}} Figure~\ref{fig:exmsl} gives an example of a $\mathcal{MSP}$ program. From this example, the following ordered list of computation kernels can be extracted:
\begin{equation*}
\Gamma = [k_0,k_1,k_2,k_3,k_4,k_5,k_6,k_7,k_8]
\end{equation*}
From this ordered list of computation kernels $\Gamma$, and from the rest of the multi-stencil program, synchronizations can be automatically detected from the call to $F_{sync}(\Gamma,0)$ to get the synchronized ordered list of kernels:
\begin{equation}
\Gamma_{sync} = [k_0,k_{0;1}^{sync},k_1,k_2,k_3,k_{1;4}^{sync},k_4,k_5,k_6,k_7,k_{7;8}^{sync},k_8],
\label{eq:exsync}
\end{equation}
\begin{subequations}
where
\begin{align}
        k_{0;1}^{sync}=(\emptyset,\{(B,nce)\},(B,\cup_{\phi \in D_1} nce(\phi))),\\
        k_{1;4}^{sync}=(\emptyset,\{(C,nec)\},(C,\cup_{\phi \in D_4} nec(\phi))),\\
        k_{7;8}^{sync}=(\emptyset,\{(I,ncc)\},(I,\cup_{\phi \in D_8} ncc(\phi))).
\end{align}
\end{subequations}

%------------------------------
\subsection{Hybrid parallelism}
A task parallelization technique is a technique to transform a program as a dependency graph of different tasks. A dependency graph exhibits parallel tasks, or on the contrary sequential execution of tasks. Such a dependency graph can directly be given to a dynamic scheduler, or can statically be scheduled. In this paper, we consider a computation kernel as a task and we introduce task parallelism by building the dependency graph between kernels of the sequential list $\Gamma_{sync}$. Thus, as $\Gamma_{sync}$ already takes into account data parallelism, we introduce hybrid parallelism.

\begin{mydef}
For two computations $k_i$ and $k_j$, with $i < j$, it is said that $k_j$ is dependent from $k_i$ with a \emph{read after write} dependency, denoted $k_i \prec_{raw} k_j$, if $\exists (r_j,n_j) \in R_j$ such that $w_i=r_j$. In this case, $k_i$ has to be computed before $k_j$.
\end{mydef}

\begin{mydef}
For two computations $k_i$ and $k_j$, with $i < j$, it is said that $k_j$ is dependent from $k_i$ with a \emph{write after write} dependency, denoted $k_i \prec_{waw} k_j$, if $w_i = w_j$ and $D_i \cap D_j \neq \emptyset$. In this case, $k_i$ also has to be computed before $k_j$.
\end{mydef}

\begin{mydef}
For two computations $k_i$ and $k_j$, with $i < j$, it is said that $k_j$ is dependent from $k_i$ with a \emph{write after read} dependency, denoted $k_i \prec_{war} k_j$, if $\exists (r_i,n_i) \in R_i$ such that $w_j=r_i$. In this case, $k_i$ also has to be computed before $k_j$ is started so that values read by $k_i$ are relevant.
\end{mydef}

Those definitions are known as \emph{data hazards classification}. However, a specific condition on the computation domain, due to the specific domain of multi-stencils, is introduced for the write after write case. One can note that the \texttt{independent} keyword of Fig.~\ref{fig:grammar} is useful in this case.

\begin{mydef}
A directed acyclic graph (DAG) $G(V,A)$ is a graph where the edges are directed from a source to a destination vertex, and where, by following edges direction, no cycle can be found from a vertex $u$ to itself. A directed edge is called an arc, and for two vertices $v,u \in V$ an arc from $u$ to $v$ is denoted $(\overset{\frown}{u,v}) \in A$.
\end{mydef}

From the ordered list of computations $\Gamma_{sync}$ and from the MSL description, a directed dependency graph $\Gamma_{dep}(V,A)$ can be built finding all pairs of computations $k_i$ and $k_j$, with $i<j$, such that $k_i \prec_{raw} k_j$ or $k_i \prec_{waw} k_j$ or $k_i \prec_{war} k_j$. 

\begin{mydef}
For two directed graphs $G(V,A)$ and $G'(V',A')$, the union $(V,A)\cup (V',A')$ is defined as the union of each set $(V\cup V', A \cup A')$.
\end{mydef}

\begin{mydef}
From the synchronized ordered list of computation kernels $\Gamma_{sync}$, the dependency graph of the computations $\Gamma_{dep}(V,A)$ is obtained from the call $F_{dep}(\Gamma_{sync},0)$, where $F_{dep}$ is the recursive function defined in Algorithm~\ref{alg:dep}.
\end{mydef}

\begin{algorithm}
\caption{$F_{dep}$ recursive function}
\label{alg:dep}
\begin{algorithmic}[1]
\Procedure{$F_{dep}$} {$\Gamma_{sync}$,$j$}
\State $k_j = \Gamma_{sync}[j]$
\If {$j=|\Gamma_{sync}|$}
\State return $(\{\},\{\})$
\ElsIf {$j<|\Gamma_{sync}|$}
\State $G=(\{\},\{\})$
\For {$0 \leq i<j$}
\State $k_i = \Gamma_{sync}[i]$
\If {$k_i \prec_{raw} k_j$ or $k_i \prec_{waw} k_j$ or $k_i \prec_{war} k_j$}
\State $G = G \cup (k_j, \{(\overset{\frown}{k_i,k_j} \})$
\EndIf
\EndFor
\State return $G \cup F_{dep}(\Gamma_{sync},j+1)$
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

This constructive function is possible because the input is an ordered list. Actually, if $k_i\prec k_j$ then $i<j$. As a result, $k_i$ is already in $V$ when the arc $(\overset{\frown}{k_i,k_j})$ is built.

One can notice that $\Gamma_{dep}$ is the dependency graph of the computations of a multi-stencil program, but it only takes into account a single time iteration. A complete dependency graph of the simulation could be built. This is a possible extension of this work.

\begin{myprop}
The directed graph $\Gamma_{dep}$ is an acyclic graph.
\end{myprop}

As a result of the hybrid parallelization, each resource $0 \leq r \leq p-1$ perform a multi-stencil program, defined by
\begin{equation*}
\mathcal{MSP}_r(\mathcal{M}_r,\Phi_r,\mathcal{D}_r,\mathcal{N},\Delta_r,T,\Gamma_{dep}).
\end{equation*}
The set of computations $\Gamma_{dep}$ is a dependency graph between computation kernels $k_i$ of $\Gamma$ and synchronizations of kernels added into $\Gamma_{sync}$. $\Gamma_{dep}$ can be built from the call to 
\begin{equation*}
F_{dep}(F_{sync}(\Gamma,0),0).
\end{equation*}

\paragraph{\textbf{Example}} Figure~\ref{fig:exmsl} gives an example of $\mathcal{MSP}$ program. From $\Gamma_{sync}$ that has been built in Equation~(\ref{eq:exsync}), the dependency DAG can be built. For example, as $k_4$ computes $F$ and $k_6$ reads $F$, $k_4$ and $k_6$ becomes vertices of $\Gamma_{dep}$, and an arc $(\overset{\frown}{k_4,k_6})$ is added to $\Gamma_{dep}$. The overall $\Gamma_{dep}$ built from the call to $F_{dep}(\Gamma_{sync},0)$ is drawn in Figure~\ref{fig:depdep}. By building synchronizations as defined in Definitions (\ref{def:sync}), (\ref{def:sync2}) and (\ref{def:sync3}), dependencies are respected. For example, $k_{0;1}^{sync}$ read and write $B$ which guarantees that $k_{0;1}^{sync}$ is performed after $k_0$ and before $k_1$.
\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   \node[] (c0) at (0,0) {$k_0$};
   \node[] (star1) at (1,0) {$k_{0;1}^{sync}$};
   \node[] (c1) at (2,0) {$k_1$};
   \node[] (c2) at (3,0.5) {$k_2$};
   \node[] (star4) at (3,1.5) {$k_{1;4}^{sync}$};
   \node[] (c3) at (3,-0.5) {$k_3$};
   \node[] (c4) at (4,0.5) {$k_4$};
   \node[] (c5) at (4,-0.5) {$k_5$};
   \node[] (c6) at (5,0.5) {$k_6$};
   \node[] (c7) at (6,0) {$k_7$};
   \node[] (star8) at (7,0) {$k_{7;8}^{sync}$};
   \node[] (c8) at (8,0) {$k_8$};
 
  \path[->]
    (c0) edge node {} (star1)
    (star1) edge node {} (c1)
    (c1) edge node {} (c2)
         edge node {} (c3)
         edge node {} (star4)
    (star4) edge node {} (c4)
    (c2) edge node {} (c4)
    (c4) edge node {} (c6)
    (c3) edge node {} (c5)
    (c5) edge node {} (c7)
    (c6) edge node {} (c7)
    (c7) edge node {} (star8)
    (star8) edge node {} (c8);
  \end{tikzpicture}
\caption{$\Gamma_{dep}$ of the example of program of Figure~\ref{fig:exmsl}}
\label{fig:depdep}
\end{center}
\end{figure}

%--------------------
\subsection{Static scheduling}
\label{sect:tsp}

In this section we detail a static scheduling of $\Gamma_{dep}$ by using minimal series-parallel directed acyclic graphs. Such a static scheduling may not be the most efficient one, but it offers a simple fork/join task model which makes possible the design of a performance model. Moreover, such a scheduling offers a simple way to propose a fusion optimization. 

In 1982, Valdes \& Al~\cite{Valdes:1979:RSP:800135.804393} have defined the class of Minimal Series-Parallel DAGs (MSPD). Such a graph can be decomposed as a serie-parallel tree, denoted $TSP$, where each leaf is a vertex of the MSPD it represents, and whose internal nodes are labeled $S$ or $P$ to indicate whether the two sub-trees form a sequence or parallel composition. Such a tree can be considered as a fork-join model and as a static scheduling. An example is given in Fig.~\ref{fig:solu}.

Valdes \& Al~\cite{Valdes:1979:RSP:800135.804393} have identified a forbidden shape, or sub-graph, called $N$, such that the following property is verified :

\begin{myth}
The transitive reduction of a DAG $G$ is MSPD if and only if it does not contain $N$ as an induced sub-graph.
\end{myth}

As $\Gamma_{dep}$ is a DAG, by removing N-Shapes it is transformed to a MSPD. The intuition is illustrated in Fig.~\ref{fig:over}. Considering the figure without the dashed line, the sub graph forms a "N" shape. The fact is that this shape cannot be represented as a composition of sequences or parallel executions.
To remove such forbidden N-shapes of $\Gamma_{dep}=(V,E)$, we have chosen to apply an over-constraint with the relation $k_0 \prec k_3$, such that a complete bipartite graph is created for the sub-dag as illustrated in Figure~\ref{fig:over}. By adding this arc to the DAG, it is possible to identify its execution as $sequence(parallel(k_0;k_2);parallel(k_1;k3))$ represented by the TSP tree of Fig.~\ref{fig:solu}.

\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   \node[] (c0) at (0,0) {$k_0$};
   \node[] (bc0) at (-1,0) {};
   \node[] (c1) at (2,0) {$k_1$};
   \node[] (c2) at (0,-1) {$k_2$};
   \node[] (bc2) at (-1,-1) {};
   \node[] (c3) at (2,-1) {$k_3$};
 
  \path[->]
    (bc0) edge [dotted] node {} (c0)
    (bc2) edge [dotted] node {} (c2)
    (c0) edge node {} (c1)
          edge [dashed] node [swap] {} (c3)
    (c2) edge node {} (c1)
        edge node {} (c3);
  \end{tikzpicture}
\caption{Over-constraint on the forbidden $N$ shape.}
\label{fig:over}
\end{center}
\end{figure}

\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   \node[] (S0) at (0,0) {$\mathcal{S}$};
   \node[] (bS0) at (-1,-1) {};
   \node[] (P1) at (-1,1) {$\mathcal{P}$};
   \node[] (aP1) at (-1.5,2) {$k_0$};
   \node[] (aaP1) at (-0.5,2) {$k_2$};
   \node[] (P2) at (1,1) {$\mathcal{P}$};
   \node[] (c1) at (0.5,2) {$k_1$};
   \node[] (c3) at (1.5,2) {$k_3$};
 
  \path[->]
    (bS0) edge [dotted] node {} (S0)
    (S0) edge node {} (P1)
         edge node {} (P2)
    (P1) edge node [swap] {} (aP1)
          edge node [swap] {} (aaP1)
    (P2) edge node {} (c1)
        edge node {} (c3);
  \end{tikzpicture}
\caption{TSP tree of Fig.~\ref{fig:over}.}
\label{fig:solu}
\end{center}
\end{figure}

After these over-constraints are applied, $\Gamma_{dep}$ is MSPD. Valdes \& Al~\cite{Valdes:1979:RSP:800135.804393} have proposed a linear algorithm to know if a DAG is MSPD and, if it is, to decompose it to its associated binary decomposition tree. As a result, the binary tree decomposition algorithm of Valdes \& Al can be applied on $\Gamma_{dep}$ to get the $TSP$ static scheduling of the multi-stencil program.

\paragraph{\textbf{Example}} From $\Gamma_{dep}$ illustrated in Fig.~\ref{fig:depdep} the TSP tree represented in Fig.~\ref{fig:tree} can be computed.

\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   %\node[circle,draw=black,fill=black,scale=0.3] (c0s) at (0,0) {};
   %\node[circle,draw=black,fill=black,scale=0.3] (c8d) at (9,0) {};

   \node[] (s8) at (4.5,0.5) {$\mathcal{S}$};
   \node[] (s9) at (3,1.5) {$\mathcal{S}$};

   %reduction c0 c1
   \node[] (s0) at (1.5,2.5) {$\mathcal{S}$};
   \node[] (s1) at (1,3.5) {$\mathcal{S}$};
   \node[] (c0) at (0.5,4.5) {$k_0$};
   \node[] (star1) at (1.5,4.5) {$k_{0;1}^{sync}$};
   \node[] (c1) at (2,3.5) {$k_1$};
   
   %reduction c7 c8
   \node[] (s2) at (7.5,1.5) {$\mathcal{S}$};
   \node[] (s3) at (7,2.5) {$\mathcal{S}$};
   \node[] (c7) at (6.5,3.5) {$k_7$};
   \node[] (star8) at (7.5,3.5) {$k_{7;8}^{sync}$};
   \node[] (c8) at (8,2.5) {$k_8$};

   \node[] (p1) at (4.5,2.5) {$\mathcal{P}$};
   %reduction c3 c5
   \node[] (s5) at (5.5,3.5) {$\mathcal{S}$};
   \node[] (c3) at (5,4.5) {$k_3$};
   \node[] (c5) at (6,4.5) {$k_0$};
   %reduction c2 c6
   \node[] (s6) at (3.5,3.5) {$\mathcal{S}$};
   \node[] (p0) at (2.8,4.5) {$\mathcal{P}$};
   \node[] (c2) at (2.3,5.5) {$k_2$};
   \node[] (star4) at (3.3,5.5) {$k_{1;4}^{sync}$};
   \node[] (s7) at (4.2,4.5) {$\mathcal{S}$};
   \node[] (c4) at (3.8,5.5) {$k_4$};
   \node[] (c6) at (4.8,5.5) {$k_6$};
 
  \path[->]
    %(c0s) edge node {} (c8d)

    (s8) edge node {} (s9)
         edge node {} (s2)
    (s9) edge node {} (p1)
         edge node {} (s0)

    %reduction c0 c1
    (s0) edge node {} (s1)
         edge node {} (c1)
    (s1) edge node {} (c0)
         edge node {} (star1)
    %reduction c7 c8
    (s2) edge node {} (s3)
         edge node {} (c8)
    (s3) edge node {} (c7)
        edge node {} (star8)

    (p1) edge node {} (s6)
         edge node {} (s5)
    %reduction c3 c5
    (s5) edge node {} (c3)
         edge node {} (c5)
    %reduction c2 c6
    (s6) edge node {} (p0)
         edge node {} (s7)
    %reduction c2 *4
    (p0) edge node {} (c2)
         edge node {} (star4)
    (s7) edge node {} (c4)
         edge node {} (c6);
  \end{tikzpicture}
\caption{Serie-Parallel tree decomposition of the example of program of Figure~\ref{fig:exmsl}}
\label{fig:tree}
\end{center}
\end{figure}

%--------------------
\subsection{Fusion optimization}
\label{sect:fusion}

Using MSL, it is possible to ask for data parallelization of the application, or for an hybrid parallelization. Even though the MSL language is not dedicated to produce very optimized independent stencil codes, but to produce the parallel orchestration of computations, building the $TSP$ tree makes available an easy optimization when the data parallelization technique is the only one used. 
This optimization consists in proposing a valid merge of some computation kernels inside a single space loop. This is called a fusion. As previously explained in Section~\ref{sect:formalism}, MSL restrict the definition of a numerical computation by writing a single quantity at a time which is more natural for numericians, and which avoid errors in manual fusion or counter-productive fusions for task parallelization. MSF guarantee that proposed fusions are correct and will not cause errors in the final results of the simulation.

Those fusions can be computed from the canonical form of the $TSP$ tree decomposition. The canonical form consists in recursively merging successive $S$ vertices or successive $P$ vertices of $TSP$.

The fusion function $F_{fus}$ is described in Algorithm~\ref{alg:fus}, where the $parent(k)$ function returns the parent vertex of $k$ in the tree, and where $k_{i;j}^{fus}$ represents the fusion of $k_i$ and $k_j$ keeping the sequential order $i;j$ if $i$ is computed before $j$ in $TSP$. Finally, $type(k)$ returns $comp$ if the kernel is a computation kernel, and $sync$ otherwise.

\begin{algorithm}
\caption{$F_{fus}$}
\label{alg:fus}
\begin{algorithmic}[1]
\Procedure{$F_{fus}$} {$TSP(V,E)$}
\For {$(k_i,k_j) \in V^2$}
\If {parent($k_i$)==parent($k_j$)}
\If {$type(k_i)==type(k_j)==comp$}
\If {parent($k_i$)==$S$}
\If {$D_i==D_j$}
\State propose the fusion $k_{i;j}^{fus}$
\Else
\If {$\exists n:D_i \rightarrow D_j \in \mathcal(N)$ and $\bigcup_{\phi \in D_i} n(\phi) = D_j$}
\State propose the fusion $k_{i;j}^{scatter}$
\EndIf
\EndIf
\ElsIf {parent($k_i$)==$P$}
\If {$D_i==D_j$ and $R_i \cap R_j \neq \emptyset$}
\State propose the fusion $k_{i;j}^{fus}$
\EndIf
\EndIf
\EndIf
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

We are not arguing that such a simple fusion algorithm could be as good as complex cache optimization techniques which can be found in stencil DSLs~\cite{spaaTangCKLL11} for example. However, this fusion takes place at a different level and can bring performance improvements as will be illustrated in Section~\ref{sect:eval}. This fusion algorithm relies on the following observations.

%JB: des items sont normalement des parties de phrases, pas de point. Ici tu peux faire des paragraphes il me semble
% \begin{itemize}
% \item
First, two successive computation kernels $k_i$ and $k_j$ which are under the same parent vertex $S$ in TSP are, by construction, data dependent. As a result, what is written by the first one is read by the second one. Thus, $w_i$ the quantity written by $k_i$ is common to these computations. Thus, if the computation domains verify $D_i=D_j$, the fusion of $k_i$ and $k_j$ will decrease cache misses.

% \item 
Second, two successive computation kernels $k_i$ and $k_j$ which are under the same parent vertex $P$ in TSP are not, by construction, data dependent. However, if the computation domains verify $D_i=D_j$, and if $R_i \cap R_j \neq \emptyset$ cache misses could also be decreased by the fusion $k_{i;j}^{fus}$.
% \end{itemize}
These two cases are illustrated by Fig.~\ref{fig:fus1} and Fig.~\ref{fig:fus2}.

\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto,every text node part/.style={align=center}]
   \node[] (S0) at (0,0) {$\mathcal{P}$};
   \node[] (ki) at (-1,-1) {$k_i$\\ $[D_i]$};
   \node[] (kj) at (1,-1) {$k_j$\\ $[D_j]$};
   \node[] (fake1) at (2,-0.5) {};
   \node[] (fake2) at (4,-0.5) {};
   \node[] (S1) at (5,0) {$\mathcal{P}$};
   \node[] (kij) at (5,-1) {$k_{i;j}^{fus}$\\ $[D_i]$};
  \path[->]
    (S0) edge node {} (ki)
         edge node {} (kj)
    (fake1) edge [bend left] node {$D_i=D_j$} (fake2)
    (S1) edge node {} (kij);
\end{tikzpicture}
\caption{First fusion case.}
\label{fig:fus1}
\end{center}
\end{figure}

\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto,every text node part/.style={align=center}]
   \node[] (S0) at (0,0) {$\mathcal{S}$};
   \node[] (ki) at (-1,-1) {$k_i$\\ $[D_i]$};
   \node[] (kj) at (1,-1) {$k_j$\\ $[D_j]$};
   \node[] (fake1) at (2,-0.5) {};
   \node[] (fake2) at (4,-0.5) {};
   \node[] (S1) at (5,0) {$\mathcal{S}$};
   \node[] (kij) at (5,-1) {$k_{i;j}^{fus}$\\ $[D_i]$};
  \path[->]
    (S0) edge node {} (ki)
         edge node {} (kj)
    (fake1) edge [bend left] node {$D_i=D_j$\\ $R_i \cap R_j \neq \emptyset$} (fake2)
    (S1) edge node {} (kij);
\end{tikzpicture}
\caption{Second fusion case.}
\label{fig:fus2}
\end{center}
\end{figure}

Thirs, and similarly to the first observation, two successive computation kernels $k_i$ and $k_j$ which are under the same parent vertex $S$ in TSP are data dependent and what is written by the first one is read by the second one. The construction of the tree also guarantees that synchronizations are not needed between these computations, otherwise a $k^{sync}$ would have been inserted between them (inherited from $\Gamma_{sync}$). Thus, $w_i$ the quantity written by $k_i$ is common to these computations. Considering the following:
\begin{itemize}
\item $D_i \neq D_j$, which means that loop fusion is by default not possible,
\item $(r_j,n_j)$ is the pair read by $k_j$ for which $r_j=w_i$ and for which $n_j:D_j \rightarrow D_i^m$
\end{itemize}
the fusion of $k_i$ and $k_j$ is possible if and only if $\exists n:D_i \rightarrow D_j \in \mathcal(N)$ such that
\begin{equation*}
\bigcup_{\phi \in D_i} n(\phi) = D_j
\end{equation*}
This means that even if domains are different, a loop fusion is possible if an adequate neighborhood function can be found. One can note that this particular fusion case is equivalent to a \emph{scatter} optimization, often used when using unstructured meshes. One can also note that the computation $k_j$ will be written in a different manner if a scatter fusion is performed or not. This is not a problem by using MSF as the fusion is proposed before the developer actually write the numerical code of $k_j$. The developer will be notified of fusions in the output of MSF. This particular case is illustrated in Fig.~\ref{fig:fus3}.

\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto,every text node part/.style={align=center}]
   \node[] (S0) at (0,0) {$\mathcal{S}$};
   \node[] (ki) at (-1,-1) {$k_i$\\ $[D_i]$};
   \node[] (kj) at (1,-1) {$k_j$\\ $[D_j]$};
   \node[] (fake1) at (2,-0.5) {};
   \node[] (fake2) at (4,-0.5) {};
   \node[] (S1) at (5,0) {$\mathcal{S}$};
   \node[] (kij) at (5,-1) {$k_{i;j}^{scatter}$\\ $[D_i]$};
  \path[->]
    (S0) edge node {} (ki)
         edge node {} (kj)
    (fake1) edge [bend left] node {$D_i \neq D_j$\\ $\exists n:D_i \rightarrow D_j$\\ $\bigcup_{\phi \in D_i} n(\phi) = D_j$} (fake2)
    (S1) edge node {} (kij);
\end{tikzpicture}
\caption{Third fusion case.}
\label{fig:fus3}
\end{center}
\end{figure}

%--------------------
\subsection{Overall compilation process}
\label{sect:ovcompil}

MSC takes a MSL file written following the grammar described in Section~\ref{sect:msl}, as well as the Generic Assembly of components represented in Fig.~\ref{fig:ga} as inputs, and generates a specialized component assembly that manages the parallel orchestration of the computations of the simulation. In this final assembly, that could be compared to a pattern or a skeleton of the simulation, the developer still has to fill-in the functions corresponding to the various computation kernels by using the DDS instantiation chosen into the specialized assembly.
The overall behavior of the compiler is as follows: %presented in Algorithm~\ref{alg:compiler}.
\begin{enumerate}
 \item it parses the MSL input file and generates $\Gamma$, the list of computation kernels,
 \item from $\Gamma$, it builds $\Gamma_{sync}$, the list including synchronizations for data parallelism using Algorithm $F_{sync}$ introduced in Section~\ref{sect:parallelism},
 \item from $\Gamma_{sync}$, it builds $\Gamma_{dep}$, the DAG supporting hybrid parallelism using Algorithm $F_{dep}$ introduced in Section~\ref{sect:parallelism},
 \item it then removes the N-Shapes from $\Gamma_{dep}$ to get a MSPD graph, and generates its serie-parallel binary tree decomposition $TSP$,
 \item it performs the fusion of kernels in $TSP$ if required (data parallelization only),
 \item it transforms GA to generate its output specialized component assembly.
\end{enumerate}

The last step of this compilation process is detailed below. It is composed of four steps:
\begin{enumerate}
 \item it instantiates $DDS$ and $Data$ components by using components implemented by a third party HPC specialist,
 \item it generates $K$ components responsible for each computation kernel of the simulation,
 \item it generates a new $Scheduler$ component,
 \item it replaces $Computations$ component by a generated sub-assembly that matches $TSP$ by using $Scheduler$, $K$ and $Sync$ components.
 \end{enumerate}

% detailed each component (interfaces)
New components have been introduced above and need to be explained. A $K$ component is a component into which the developer will write numerical code. It could represents a single computation kernel described by the numerician using MSL, or it could represents the fusion of multiple computation kernels. In any case the name of the generated component will use kernel identifiers used in the MSL description. A $K$ kernel is composed of $m$ use ports that are used to be connected to the $m$ quantities needed by the computation (\ie the numerical code). The component also exposes a provide port to be connected to the $Scheduler$ component. A generic $K$ component is represented in Fig.~\ref{fig:k}.

A $Sync$ component is a static component (not generated) composed of a use-multiple port which is used to request synchronizations for all quantities it is linked to ($Data$). The component also exposes a provide port to be connected to the $Scheduler$ component. The $Sync$ component is represented in Fig.~\ref{fig:sync}.

Finally, the $Scheduler$ component is the component responsible for implementing the $TSP$ computed by MSC. Thus, this component represents the specific parallel orchestration of  computations. It exposes as many use ports as there are instances of $K$ components to call (\ie computations and fusions of computations). The component also exposes a provide port to be connected to the $Time$ component. A generic $Scheduler$ component is represented in Fig.~\ref{fig:ass}.

% draw the sepcialized assembly of the example
To illustrate how a specialized assembly is generated, the specialized assembly of the example that has been used throughout this paper is represented in Fig.~\ref{fig:specass}. 

\begin{figure}[t]
\begin{center}
\subfloat[][$K$\label{fig:k}]{
\begin{tikzpicture}[shorten >=1pt, node distance=1cm, on grid, auto]
   \node[component] (seq) at (0,0) {$K$}; 
   \node[provide] (p) at (-1,0) {}; 
   \node[use] (u) at (1,0) {}; 
   \node[] (star) at (1,0.5) {$*$};
  \path[-]
    (p) edge node {} (seq)
    (seq) edge node {} (u);
\end{tikzpicture}
}
\hspace{\fill}
\subfloat[][\label{fig:sync}$Sync$]{
\begin{tikzpicture}[shorten >=1pt, node distance=1cm, on grid, auto]
   \node[component] (seq) at (0,0) {$Sync$};
   \node[provide] (p) at (-1,0) {};
   \node[use] (u) at (1,0) {$m$};
 
  \path[-]
    (p) edge node {} (seq)
    (seq) edge node {} (u);
\end{tikzpicture}
}
\hspace{\fill}
\subfloat[][\label{fig:sched}$Scheduler$]{
\begin{tikzpicture}[shorten >=1pt, node distance=1cm, on grid, auto]
   \node[component] (sync) at (0,0) {$Scheduler$};
   \node[provide] (p) at (-1.5,0) {};
   \node[use] (u) at (1.5,0) {};
   \node[] (star) at (1.5,0.5) {$*$};
  \path[-]
    (p) edge node {} (sync)
    (sync) edge node {} (u);
\end{tikzpicture}
}
\caption{Specific components used to transform GA to the specialized component assembly of the simulation.}
\label{fig:specass}
\end{center}
\end{figure}

\begin{figure}[t]
\begin{center}
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   \node[component,fill=green!50!white] (D) at (0,0) {$Driver$};
   \node[provide] (Dp) at (-1,0) {};
   \node (Ds) at (-1.5,0) {start};
   \node[use,right=1.5cm of D] (Du1) {};
   \node[use,below=1.75cm of D] (Du2) {};
   \node[use,below=0.8cm of Du1] (Du3) {$m$};

   \node[provide,below=0.15 of Du2] (Tp) {};
   \node[component,below=1.6cm of Tp,fill=green!50!white] (T) {$Time$};
   \node[use,right=1cm of T] (Tu) {};

   \node[provide,right=0.15 of Tu] (Cp) {};
   \node[component, fill=green!50!white,right=2cm of Cp] (C) {$Scheduler$};
   \node[use,right=1.8 of C] (scu1) {};
   \node[use,below=0.2 of scu1] (scu2) {};

   \node[provide,below right=0.2 of Du3] (Datap1) {};
   \node[component,draw=red,line width=0.4mm,fill=green!50!white,above=0.8cm of Datap2] (Data) {$A$};
   \node[component,draw=red,line width=0.4mm,fill=green!50!white,below=0.8cm of Data] (Data2) {$B$};
   \node[below=0.5cm of Data2] (Data3) {...};
   \node[use,above=0.8cm of Data] (Datau) {};
   \node[provide,right=1cm of Data] (mpidata) {};
   \node[right=0.2 of mpidata] (u1) {1};
   \node[provide,right=1cm of Data2] (mpidata3) {};
   \node[right=0.2 of mpidata3] (u2) {2};
   \node[provide,below=0.2cm of mpidata3] (mpidata2) {};
   \node[right=0.2 of mpidata2] (u3) {3};

   \node[provide,right=0.15 of Du1] (DDSp1) {};
   \node[provide,above=0.15 of Datau] (DDSp2) {};
   \node[component, draw=red, fill=red!50!white,above=0.8cm of DDSp2] (DDS) {cart};
   
   \node[provide,right=0.15 of scu1] (pk0) {};
   \node[provide,right=0.15 of scu2] (psync1) {};
   \node[component, fill=blue!50!white,right=2cm of pk0] (k0) {$k_0$};
   \node[use,right=0.8 of k0] (k0u) {};
   \node[right=0.2 of k0u] (1) {1};
   \node[use,below=0.2 of k0u] (k0u2) {};
   \node[right=0.2 of k0u2] (1) {2};
   \node[component, fill=green!50!white,below=0.8cm of k0] (sync1) {$Sync(0,1)$};
   \node[use,right=1.5 of sync1] (syncu) {};
   \node[right=0.2 of syncu] (1) {3};
   \node[below=0.5cm of sync1] (next) {...};
 
  \path[-]
    (Dp) edge node {} (D)
    (D) edge node {} (Du1)
        edge node {} (Du2)
        edge node {} (Du3)
    (DDSp1) edge node {} (DDS)
    (Tp) edge node {} (T)
    (T)  edge node {} (Tu)
    (Cp) edge node {} (C)
    (C) edge node {} (scu1)
    	edge node {} (scu2)
    (Datap1) edge node {} (Data.west)
    		edge node {} (Data2.west)
    (Data) edge node {} (Datau)
          edge node {} (mpidata)
     (Data2) edge node {} (mpidata3)
     		edge node {} (mpidata2)
    (DDSp2) edge node {} (DDS)
    (pk0) edge node {} (k0)
    (psync1) edge node {} (sync1)
    (k0) edge node {} (k0u)
    	edge node {} (k0u2)
    (sync1) edge node {} (syncu)
    (Data2.west) edge [bend left] node {} (Datau);
    %(k0p.east) edge [bend right] node {} (mpidata.east)
    %(k0p2.east) edge [bend right] node {} (mpidata2.east)
    %(syncp.east) edge [bend right] node {} (mpidata3.east);
\end{tikzpicture}
\vspace*{.5em}
\caption{Generic Assembly according to the Multi-Stencil program formalism.
The complete assembly would be too complex to entirely draw and only a sub part is represented.
Some connections are represented by numbers instead of lines in order not to make the schema more clear.
\JB{etre plus clair sur les notations (Comme pour la Fig\ref{fig:ga}}}
\label{fig:specass}
\end{center}
\end{figure}

%--------------------
\subsection{Performance model}
\label{sect:perfs}

In this subsection we introduce two performance models, one for the data parallelization technique, and one for the hybrid data and task parallelization technique, both previously explained.

The performance model for the data parallelization technique is inspired by the Bulk Synchronous Parallel model.
We consider that each process handles its own sub-domain that has been distributed in a perfectly balanced way.
The performance model describes the computation time as the sum of the sequential time divided by the number of processes, and of the time spent in communications between processes. Thus, for
\begin{itemize}
\item $T_{SEQ}$ the sequential reference time, 
\item $P$ the total number of processes, 
\item $T_{COM}$ the communications time, 
\end{itemize}
the total computation time is
\begin{equation}
T = \frac{T_{SEQ}}{P} + T_{COM}.
\end{equation}

Thus, when the number of processes $P$ increase in data parallelization, the performance model limit is $T_{COM}$
\begin{equation}
\lim\limits_{P \rightarrow +\infty} T = T_{COM}.
\end{equation}

As a result, the critical point for performance is when $T_{COM} \geq \frac{T_{SEQ}}{P}$, which happens naturally in data parallelization as $T_{COM}$ will increase with the number of processes, and $\frac{T_{SEQ}}{P}$ decrease with the number of processes.
\JB{Pour moi $T_{COM}$ est constant quand on communique entre voisins... Il diminue meme legerement si on reduit les donnees par process.}

This limitation is always true, but can be delayed by different strategies. First, it is possible to overlap communications and computations. Second, it is possible to introduce another kind of parallelization, task parallelization. Thus, for the same total number of processes, only a part of them are used for data parallelization, and the rest are used for task parallelism. As a result, $\frac{T_{SEQ}}{P}$ will continue to decrease but $T_{COM}$ will increase later. This second strategy is the one studied in the following hybrid performance model.

For an hybrid (data and task) parallelization technique, and for
\begin{itemize}
\item $P_{data}$ the number of processes used for data parallelization,
\item $P_{task}$ the number of processes used for task parallelization, such that $P = P_{data} \times P_{task}$ is the total number of processes used,
\item $T_{task}$ the overhead time due to task parallelization technique,
\item and $F_{task}$ the task parallelization degree of the application,
\end{itemize}
the total computation time is
\begin{equation}
T = \frac{T_{SEQ}}{P_{data} \times F_{task}} + T_{COM} + T_{task}
\end{equation}

The time overhead due to task parallelization can be represented as the time spent to create a pool of threads and the time spent to synchronize those threads. Thus, for
\begin{itemize}
\item $T_{cr}$ the total time to create the pool of threads (may happened more than once), 
\item $T_{sync}$ the total time spent to synchronize threads, 
\end{itemize}
the overhead is
\begin{equation*}
T_{task} = T_{cr} + T_{sync}.
\end{equation*}

The task parallelization degree of the application $F_{task}$ is the limitation of a task parallelization technique. As explained before, a task parallelization technique is based on the dependency graph of the application. Thus, this dependency graph must expose enough parallelism for the number of available threads. For this performance model we consider that 
\begin{equation*}
F_{task} = P_{task}, 
\end{equation*}
however, as it will be illustrated in Section~\ref{sect:eval} $F_{task}$ is more difficult to establish. Actually, the lower and upper bounds of $F_{task}$ are constrained by the dependency graph of the application.

\medskip
As a result when $P_{data}$ is small a data parallelization technique may be more efficient, while an hybrid parallelization could be interesting at some point to improve performance. The question is: when is it interesting to use hybrid parallelization ? This paper does not propose an intelligent system to answer this question automatically, however, it offers a way to understand how to answer the question. To answer this question let's consider the two parallelization techniques, data only and hybrid. We denote
\begin{itemize}
\item $P_{data1}$ the total number of processes entirely used by the data only parallelization,
\item $P_{data2}$ the number of processes used for data parallelization in the hybrid parallelization,
\item and $P_{task}$ the number of processes used for task parallelization in the hybrid parallelization,
\item such that $P_{data1} = P_{data2} \times P_{task}$.
\end{itemize}

We search the point where the data parallelization is less efficient than the hybrid parallelization. Thus, 
\begin{equation*}
\frac{T_{SEQ}}{P_{data1}} + T_{COM1} \geq \frac{T_{SEQ}}{P_{data2} \times P_{task}} + T_{COM2} + T_{task}.
\end{equation*}

This happens when
\begin{equation}
T_{COM1} \geq T_{COM2} + T_{task}
\label{eq:hyb}
\end{equation}

This performance model will be validated and will help explain results of Section~\ref{sect:eval}.