To numerically solve a set of PDEs, iterative methods are frequently used to approximate the solution more easily by a discretization of the time and the space (which is called a mesh). Thus, the PDEs are transformed to a set of numerical computations applied at each time step and on all elements of the mesh. Among the numerical computations is found a set of numerical schemes, also called stencil computations, and a set of auxiliary numerical computations also needed to perform the simulation. 
%A stencil is a computation applied, at each time iteration, to a complete set of elements of a mesh. 
In this paper is distinguished the concept of stencil shape (often called a stencil) from the concept of stencil computation. A stencil computation involves values of a simulated quantity (pressure, speed etc.) on a given element of the mesh and also on a given neighborhood of the element, denoted $\mathcal{N}$. A stencil shape is the shape of the needed neighborhood $\mathcal{N}$ in a stencil computation. As a result, a mesh-based numerical simulation is composed of a certain number of stencil computations, which is called in this paper \textit{multi-stencils}, involving one or more stencil shapes. In applied mathematics, an explicit numerical scheme is typically a stencil computation, but, on the other hand, an implicit numerical scheme also requires the resolution of a linear system at each time iteration. This last kind of scheme is above the scope of this work, however explicit schemes already represent a huge part of PDE solvers. In this section is defined the formalism of a \textit{stencil program}, a description of how to parallelize such programs and the associated definitions of a \textit{dependency} and a \textit{synchronization}. %Finally, a definition of a \textit{phase}, a \textit{group}, and a \textit{kernel}, from which numerical computations of a stencil program can be classified for any kind of mesh, and from which different levels of parallelism can be detected, are given.

%-------------------------------------
\subsection{Definitions}

A stencil program or application is a program in which stencil expressions (or computations) are computed among other kind of numerical expressions, which do not involve neighborhood information. If the space domain is denoted $\Omega$, the time domain $\tau$, a stencil program is denoted as 
\begin{equation}
\mathcal{P}(\mathcal{M},\Delta,\Gamma,\mathcal{T}),
\end{equation}
where $\mathcal{M}$ is a mesh which discretizes $\Omega$, $\Delta$ is the set of quantities and auxiliary data mapped onto $\mathcal{M}$, $\Gamma$ is the ordered set of numerical computations in the program, and $\mathcal{T}$ is the discretization of the time domain $\tau$. $\mathcal{M}$ can be considered as a set of different sets of elements $\{E_0,...,E_n\}$ and a connectivity between them. For example, $\mathcal{M}$ could be composed of $E_0$ the set of cells, $E_1$ the set of nodes and $E_2$ the set of edges, such that each cell is composed of four edges, and each edge is composed of a source and a destination node. 
%If the operator $A(B)$ means that $A$ is applied on $B$, a 
A stencil program is composed of a set of quantities to simulate (pressure, speed etc.), mapped on the mesh, which is denoted $\Delta(E_k), 0 \leq k \leq n$. For all $t \in \mathcal{T}$ the set of numerical computations are applied on the data, which is simply denoted by $\Delta=\Gamma(\Delta)$.%such that for $\Delta_1, \Delta_2 \subset \Delta$, $\Delta(t) = \Gamma(\Delta_1(t-1),\Delta_2(t))$. 
In a mathematical numerical expression, a quantity to simulate has $d$ dimensions for the space domain and one additional dimension for the time, for example $\mathcal{A}(x,y,z,t)$. However, the implementation of a quantity in $\Delta$ to a data structure $\delta$ does not represent the time dimension in the rest of this paper. Thus, for a given quantity $\mathcal{A}$ at $t$ and $t-1$, eventually two different data structures (arrays) $\delta, \delta'$ are used. %As a result, the expression of a computation can be simplified to $\Delta=\Gamma(\Delta_1)$, which means that in a stencil program, the set of quantities $\Delta$ are computed from a given subset of quantities $\Delta_1$ on which is applied the set of computations.

A numerical computation in $\Gamma$ is denoted 
\begin{equation}
c(R,w,D,e), 
\end{equation}
where $R \subset \Delta, w \in \Delta$ are respectively the set of data read and written during the numerical expression $e$ (a single data is written in a single computation), and $D$ is one of the subsets $E_i \subset \mathcal{M}$. It has to be noticed that during a numerical simulation, at each time iteration, all the elements of a mesh are computed. However, it happens that the computation of the mesh elements is splitted in different computations (for example the computation of the physical border). In this case additional $E_i$ can be specified for the mesh $\mathcal{M}$. %For this reason, the sub-domain concept is introduced. Thus, i
In a computation $c$, $\forall d \in D$, the numerical expression $e(d)$ is applied. If the number of computations in a stencil program $\mathcal{P}(\mathcal{M},\Delta,\Gamma,\mathcal{T})$ is $card(\Gamma)=m$, such that $\bigcup_{i=0}^{m-1}c_i = \Gamma$, then $\bigcup_{i=0}^{m-1}R_i \cup w_i = \Delta$. %Finally, in this work, and because of the choice to not represent the time dimension in a data $\delta \in \Delta$, a computation $c(R,w,D,e)$ always verifies $R \cap w = \emptyset$ otherwize an implicit numerical scheme has to be solve (a relation between a $\mathcal{A}(t)$ and $\mathcal{A}(t)$ itself). 

Two different types of computations can be handled by a stencil program: stencil computations or numerical computations which do not involve neighborhood information (as $map$ or $zip$ in functionnal languages). The needed neighborhood $\mathcal{N}$ of a stencil computation is the shape of the stencil. For regular meshes (as for example in finite difference methods), a stencil shape is not difficult to express, however, in the general purpose, the neighborhood is specific to the kind of mesh it is applied on, and could be difficult to express. For example, the neighborhood for unstructured meshes is composed of different concepts which are explained by the operators of Laszlo and Dobkin~\cite{}. This paper gives a general definition of a stencil computation, with a generic vision of a neighborhood $\mathcal{N}$. %independently of a specific neighborhood shape which depends on the mesh used in the simulation. 
Actually, the solution presented in this paper aims to handle any kind of mesh. A stencil computation is denoted by the general notation
\begin{equation}
s(R,w,D,e,\mathcal{N}),
\end{equation}
where $R \subset \Delta, w \in \Delta$ are read and written in the numerical expression $e$, which uses a given neighborhood function denoted $\mathcal{N}$ on a sub-domain $D$. In a stencil computation $s$, $\forall d \in D$, the numerical expression is applied such that $w(d) = e(R(d),R(\mathcal{N}(d))$. Finally, in this work, a stencil computation $s(R,w,D,e,\mathcal{N})$ always verifies $R \cap w = \emptyset$, otherwize an implicit numerical scheme has to be solve (a relation between a a quantity $\mathcal{A}(t)$ and $\mathcal{A}(t)$ itself at the same time iteration).

The second kind of numerical computation is denoted
\begin{equation}
l(R,w,D,e).
\end{equation}
This is a local numerical expression, where $e$ does not involve $\mathcal{N}(d)$, and where $e$ is applied such that $w(d) = e(R(d))$.

%-------------------------------------
\subsection{Stencil program parallelization}
\label{sect:parall}
Mesh-based numerical simulation can be parallelized in various ways and is an interesting kind of application to take advantage of modern heterogeneous HPC architectures, mixing clusters, multi-cores CPUs, vectorization units, GPGPU and many-core accelerators.

Maybe the more important parallelization technique of such simulations is called \textit{SPMD} (Single Program, Multiple Data) which can also be called \textit{coarse-grain data parallelism}. This concept represents the well-known and broadly-used domain decomposition technique to split the mesh in balanced sub-meshes for available resources. Because of the good locality degree of numerical simulations, only a small number of additional elements, called a \textit{recorvery} or a \textit{ghost} area, are needed to exchange information computed by another resource to correctly compute stencils. As the amount of communications of such parallelization are rather small compared to the amount of computations (small neighborhood), and also by the addition of recovery of communications by computations, this parallelization technique has proved many times its efficiency and scalability on numerical simulations~\cite{}. This parallelization technique can be applied on shared and distributed memory architectures (clusters, multi-core CPUs), and it stays the most efficient parallelization technique if the numerical simulation has to be executed on a distributed memory architecture. For this reason, this parallelization stays a reference in the domain and is still studied for more complex simulations, using unstructured meshes, multi-grids techniques etc.

However, because of energy consumption constraints, most of the \textit{top500}~\footnote{http://www.top500.org} supercomputer centers are not only composed of clusters of multi-core machines, but also of accelerator cards such as GPGPU and many-cores. In addition to this, vectorization units are also available on most CPUs. This introduces another kind of parallelization, linked to the first one, called in this paper \textit{fine-grain data parallelism}, which is composed of \textit{SIMD} (Single Instruction, Multiple Data) and \textit{SIMT} (Single Instruction, Multiple Threads) parallelization techniques. Such techniques typically compute a numerical expression in parallel on a vector, using vectorization units or threads, which perfectly matches the type of computation in the nested loops of numerical simulations.

Finally, the last parallelization technique which can be applied on various architectures (clusters, multi-cores, many-cores, GPGPU), is the \textit{task parallelism} technique. Instead of spliting the data of the simulation (or the vector used in a numerical expression), the different tasks of the simulation are identified and scheduled over the available resources during the execution (at runtime)~\cite{}. As numerical computations of a simulation can be considered as tasks, linked together through data dependencies for example, this parallelization technique can be used for HPC numerical simulations too.

This section discussed the various parallelization techniques which can be used for numerical simulations. As a result, it seems that, by a combination of existing techniques, modern heterogeneous parallel architectures can be fully harnessed by such applications~\cite{}. In this paper is presented a component-based model to expose those three kinds of parallelism in the overall simulation, while hiding them from the user (numericians).

%-------------------------------------
\subsection{Dependencies and synchronizations}
\label{sect:dep}
To define and explain the component-based implicit parallelism model presented in this paper, two additional definitions handle the relations between data read and written into computations. Those relations are responsible for dependencies and synchronizations.

\medskip
\begin{mydef}
\textit{For two computations $c_1(R_1,w_1,D_1,e_1)$ and $c_2(R_2,w_2,D_2,e_2)$, it is said that $c_2$ is dependant from $c_1$, denoted $c_1<c_2$, if $w_1 \cap R_2 \neq \emptyset$. In this case, $c_1$ has to be computed before $c_2$. The binary relation $c_1<c_2$ represents a \textit{dependency}.}
\end{mydef}

\medskip
Dependencies lead to one computation before another, but when the program is parallelized some dependencies are also responsible for synchronizations between different processes. The opposite definition can also be given, 

\medskip
\begin{mydef}
\textit{For two computations $c_1(R_1,w_1,D_1,e_1)$ and $c_2(R_2,w_2,D_2,e_2)$, it is said that $c_2$ is independant from $c_1$, denoted $c_1 \not <c_2$ or $c_1 \parallel c_2$, if $w_1 \cap R_2 = \emptyset$.}
\end{mydef}

Considering a coarse-grain data parallelized stencil program $\mathcal{P}$,

\medskip
\begin{mydef}
\textit{A depedency between two computations $c_1<c_2$ is a \textit{synchronisation}, denoted $c_1 \ll c_2$, if $c_2=s_2(R_2,w_2,D_2,e_2)$, and $w_1 \cap R_2 \neq \emptyset$.}
\end{mydef}

\medskip
Actually, in a parallel stencil program, synchronizations between processes are always needed for the set of data $R$ of a stencil $s(R,w,D,e)$ because the expression $e$ involves a neighborhood $\mathcal{N}$ which could be handled by another processor or resource.


