%-------------------------------------
\subsection{Phase, group and kernel}
Using the formalism described in Sections~\ref{sect:formalism} and~\ref{sect:component}, the following definitions are given for the solution.

\medskip
\begin{mydef}
For a stencil program $\mathcal{P}(\mathcal{M},\Delta,\Gamma,\mathcal{T})$, a \emph{phase} $\Phi \subset \Gamma$ is a subset of ordered computations $\{c_0,...,c_{n-1}\} \in \Gamma$ such that $\forall 0 \leq i,j \leq n-1, \nexists c_i,c_j \in \Phi$, which verifies $c_i \ll c_j$.
\end{mydef}

One can notice, however, that in a phase $Phi$ of a stencil program it is possible to have a pair $c_i,c_j \in \Phi$, which verifies $c_i<c_j$. As a result, a phase is equivalent to a sequence of computations $SEQ$. The set of phases of a stencil program are ordered and a synchronization is needed between two different \emph{phases} of a stencil program $\mathcal{P}$. In other terms for two phases $\Phi_1$ and $\Phi_2$ $\exists c_i \in \Phi_1, c_j \in \Phi_2$ such that $c_i \ll c_j$. If $m$ is the number of stencil computations in a phase $\Phi_i$ among $n$ total computations, the synchronizations of $\Phi_i$ are applied on $\bigcup_{j=0}^{m-1}R_j$. 

Instead of introducing an additional definition for the equivalent of $SSEQ$, and because $sequence(sequence(a,b),sequence(c,d))=sequence(a,b,c,d)$, synchronizations needed by a new phase are commputed inside the Phase itself before computations. A phase component is defined as
\begin{equation}
Phase(\{provide,use-multiple, synchronization\},\{phase\}).
\end{equation}
 The function $phase$ performs the needed synchronizations before to use each component of the sequence

\begin{algorithm}[H]
 synchronizations\\
 \ForAll{component $cp$ to use}{
 use the provide interface of $cp$
 }
 \caption{phase function}
 \end{algorithm}

\begin{mydef}
For a stencil program $\mathcal{P}(\mathcal{M},\Delta,\Gamma,\mathcal{T})$, a \emph{group} $\mathcal{G} \subset \Gamma$ is a subset of unordered computations $\{c_0,...,c_{n-1}\}$ such that $\forall 0 \leq i,j \leq n-1, \nexists c_i,c_j \in \mathcal{G}$, which verifies $c_i<c_j$.
\end{mydef}

Thus, a dependency exists between two different \emph{groups} of a stencil program $\mathcal{P}$, but the computations inside a \emph{group} are unordered, without dependencies. As a result, the Group component is defined as $PAR$
\begin{equation}
Group(\{provide,use-multiple\},\{group\}),
\end{equation}
 where the function $group$ creates a thread for each component to use, and join all threads at the end.

\begin{algorithm}[H]
 \ForAll{component $cp$ to use}{
 create thread $t$
 $t$ uses the provide interface of $cp$
 }
 join all threads
 \label{alg:par}
 \caption{group function}
 \end{algorithm}

\begin{mydef}
For a stencil program $\mathcal{P}(\mathcal{M},\Delta,\Gamma,\mathcal{T})$, a \emph{kernel} $\mathcal{K} \subset \Gamma$ is a subset of unordered computations $\{c_0,...,c_{n-1}\}$ such that $\forall 0 \leq i,j \leq n-1, \nexists c_i,c_j \in \mathcal{K}$, which verifies $c_i<c_j$, and where $\forall i,j<n, D_i=D_j$.
\end{mydef}

\medskip
Thus, a \emph{kernel} could contains $n$ computations $\{c_0,...,c_{n-1}\}$, but handles a single space loop in a coarser computation $c'$ such that $c'(\bigcup_{i=0}^{n-1}R_i,\bigcup_{i=0}^{n-1}w_i,D,\{e_0,...,e_{n-1}\})$. This definition differs from the component $K$ described in this section as a kernel could contains more than one computation if the space domain is the same. This definition of a kernel improves performances as the memory bandwidth is better used. A kernel component if defined as
\begin{equation}
Kernel(\{provide\},\{kernel\}).
\end{equation}
 The function $kernel$ performs the set of numerical computations

\begin{algorithm}[H]
 \ForAll{elements $d$ in $D$}{
 $w(d)=e_1(R(d),R(\mathcal{N}(d)))$\\
 $w(d)=e_2(R(d),R(\mathcal{N}(d)))$\\
 ...
 }
 \caption{kernel function}
 \end{algorithm}

\medskip
The component assembly of the computations of a mesh-based numerical simulation using phase, group and kernel components is represented in the Figure~\ref{phgpk}. This assembly is equivalent to the one presented in the Figure~\ref{approx}, but $SSEQ$ and $SEQ$ are fusionned in $phase$, and the component $kernel$ is bit different from $K$.

%---------
\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   \node[component] (phase) at (0,0) {phase};
   \node[component] (group) [right=of phase] {group};
   \node[component] (kernel) [right=of group] {kernel};
 
  \path[->]
             (phase)  edge  [connection]               node           {1,n}   (group)
                      edge [mpiconn] node {} (-1,0)
             (group)   edge [connection]          node       {1,n}   (kernel)
             (phase)   edge [bend left=100,connection]   node  [swap]   {1,n}   (kernel);
\end{tikzpicture}
\caption{General assembly proposed in our solution}
\label{phgpk}
\end{center}
\end{figure}

This component assembly of computations can be, by definition, extracted from an ordered list of typed computations $s(R,w,D,e,\mathcal{N})$ or $l(R,w,D,e)$. $R$, $w$ and $D$ are used to build Phases, Groups and Kernels, while $e$ and $\mathcal{N}$ are used inside the Kernel components. As a result, we can define a function $\lambda$ to build the computation assembly
\begin{equation}
\lambda : list_o(c(R,w,D,e)) \rightarrow \alpha ({{Phase},{Group},{Kernel}},L,R)
\end{equation}

%-------------------------------------
\subsection{Overall assembly}

\paragraph{The SIPSim model.} The SIPSim model~\cite{} is a model which proposes a systematic way to create an SPMD implicit parallelism solution for mesh-based numerical simulations through four concepts. The first concept is a \textit{Distributed Data Structure} (DDS), which represents the distributed mesh. Thus, the DDS manages a domain decomposition, or more broadly a graph partitioning problem which balances the number of elements on available resources, and which tries to minimize the number of needed synchronizations between resources. It has to be noticed that this particular task could be managed by an external graph partitionner. In addition to this, the DDS also has to offer an efficient access to an element of a mesh and to its neighborhood. The second concept is the \textit{Distributed Property Map} (DPMap), which is responsible to map a quantity to simulate (a data) onto the distributed mesh. The third concept is an \textit{applicator}, which is responsible for applying a set of synchronizations before a set of numerical computations written by the user (also called operations). Finally, the last concept contains the \textit{interfaces} needed to write an operation in a sequential programming style while using DPMaps mapped onto the DDS. Those interfaces are directly linked to the type of DDS used.

\paragraph{CSM overall assembly.} From the SIPSim model, CSM uses the concept of DDS and DPMap and transforms them in components to build the overall component assembly. The implementation of those components are not described in this paper, and it is possible to imagine an adhoc implementation in simple cases, as a Cartesian mesh, or existing external implementations (libraries) in more complex cases. This choice is an implementation choice and will be compatible with CSM. In addition to those two new components, the concept of \textit{interface} of the SIPSim model has to be kept to write the implementation of the expressions $e$ of the numerical computations on distributed data structures. But this concept is not needed as a component. It could be provided by the DDS itself or it could be introduced as a header file for example.

The overall assembly produced by the model is presented in the Figure~\ref{csmassembly}. First, the component \textit{DDS} and the component \textit{Data}, which corresponds to the DPMap concept of the SIPSim model, are introduced in the assembly with the same functionnality than described above. 

Second, one can notice that the assembly described in the previous Figure~\ref{phgpk} is, of course, inserted in the overall assembly. However, even if the \textit{Phase} component is still responsible for starting a set of synchronizations, those synchronizations are in fact performed by the \textit{Connector} component. Actually, the \textit{Phase} component uses the data concerned by its synchronizations, and then each data uses the \textit{Connector}, which finally performs the synchronizations. As a result, the \textit{Connector} component can be implemented with threads, or with MPI, without the modification of \textit{Phase} and \textit{Data} components. This is a separation of concerns between the data of the simulation and the parallelization strategy which increases the portability of the application. This strategy has been more described in the paper~\cite{}. 

The third important point to notice is the introduction of the component \textit{Time} which is responsible for the time iteration steps of the simulation. To establish the end of time steps, most of the time a convergence function is computed on data, but it also happens that a fix number of iterations is precised. For this reason, the instanciation of the component \textit{Convergence} is optional. The \textit{Driver} component starts the simulation and launches the initialization of the DDS, and the \textit{DrApp} component starts the initialization of the data and also the time loop of the simulation. Finally, the \textit{Initializer} component is linked to the initialization of data (read a file format, default value, or numerical computation). The computation components, specific to each simulation, are \textit{Kernel} and \textit{Convergence}, and to perform numerical expressions $e$ those components have to be connected to the Data of each $R$ and $w$.

%---------
\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   \node[component] (phase) at (0,0) {Phase};
   \node[component] (group) [right=of phase] {Group};
   \node[component] (kernel) [right=of group] {Kernel};
   \node[component] (init) [above=1.5cm of group] {Initializer};
   \node[component] (data) [above=1.5cm of phase] {Data};
   \node[component] (time) [left=of phase] {Time};
   \node[component] (drapp) [above=1.5cm of time] {DrApp};
   \node[dcomponent] (conv) [left=of time] {Convergence};
   \node[component] (dr) [above=1.5cm of drapp] {Driver};
   \node[component] (dds) [above=1.5cm of data] {DDS};
   \node[component] (connec) [above=1.5cm of init] {Connector};
 
  \path[->]
  	(dr)		edge  [connection]	node	{}	(dds)
  	(dr)		edge  [connection]	node	{}	(drapp)
  	(drapp)		edge  [connection]	node	{1,n}	(data)
  	(drapp)		edge  [connection]	node	{}	(time)
  	(data)  	edge  [connection]	node	{}	(dds)
  	(data)  	edge  [connection]	node	{}	(connec)
  	(connec)	edge  [connection]	node	{}	(dds)
  				edge  [mpiconn]		node	{}	(4,3)
  	(data)		edge  [connection]	node	{1,n}	(init)
  	(time)		edge  [connection]	node	{}	(conv)
  	(time)		edge  [connection]	node	{1,n}	(phase)
  	(phase)		edge  [connection]	node	{1,n}	(data)
  	(phase)		edge  [connection]	node	{1,n}	(group)
  	(phase)		edge  [bend right=50,connection]   node  [swap]   {1,n}   (kernel)
  	(group)		edge  [connection]	node	{1,n}	(kernel)
  	(kernel)	edge  [connection]	node	{1,n}	(data)
  	(conv)	edge  [connection,dashed]	node	{1,n}	(data);
\end{tikzpicture}
\caption{Overall assembly of the Component Stencil Model.}
\label{csmassembly}
\end{center}
\end{figure}

This overall assembly can be compiled from a set of sequential and descriptive information. First an unordered list of data is needed. A data is defined as
\begin{equation}
data(init_f,E_i,type)
\end{equation}
, where $init_f$ is the function to initialize the data (in the Initializer component), $Ei$ is the set of elements of the mesh on which the data is applied, and $type$ is the actual type of data ($int$, $double$ etc.).

The second needed information is the description of the mesh to use as
\begin{equation}
mesh(type,dimension,size)
\end{equation}
, where $type$ of is the topology of the mesh (for example Cartesian or k-sided unstructured meshes), $dimension$ is the dimension of the mesh, and $size$ is the size of the mesh.

As a result, we can define a function $\Lambda$ to build the overall simulation assembly of the Figure~\ref{csmassembly} as
\begin{equation}
\Lambda : list_o(c(R,w,D,e)) \times list(data(init_f,E_i,type)) \times mesh(type,dimension,size) \rightarrow \alpha (\mathcal{C},L,R)
\end{equation}

The needed information of the function $Lambda$ are typically produced by numericians who try to solve a set of partial differential equations by existing numerical methods, by a combination of methods, or by their own methods. Once the parallel assembly $\alpha$ is created, the engineer or a numerican developper of the lab can write the set of numerical computation components $Kernel$, the set of $Initializer$ components, and eventually the $Convergence$ component. All those components are written with an imperative sequential programming style using the interfaces linked to the DDS. As a result, multiple levels of separation of concerns are introduced. First the separation of concerns between the numerician and the person in charge of the implementation, and second the separation of concerns between the implementation and the parallelization of the simulation, which is generated.

%\paragraph{Evaluation.}
% for different kind of mesh
