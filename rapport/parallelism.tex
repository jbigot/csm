Multi-stencil mesh-based numerical simulation can be parallelized in various ways and is an interesting kind of application to take advantage of modern heterogeneous HPC architectures, mixing clusters, multi-cores CPUs, vectorization units, GPGPU and many-core accelerators.

%------------------------------
\subsection{Coarse-grain data parallelism}
In a data parallelization technique, the idea is to split the data on which the program is computed in balanced sub-parts, one for each available resource. The same sequential program can afterwards be applied on each sub-part simultaneously, with some additioinal synchronizations between resources to update the data not computed locally, and thus to guarantee a correct result.

More formally, the data parallelization of a multi-stencil program $\mathcal{MSP}(T,\mathcal{M},\Delta,\Gamma)$ consists in, first, a partitioning of the mesh $\mathcal{M}$ in $p$ balanced sub-meshes (for $p$ resources) $\{\mathcal{M}_0,\dots,\mathcal{M}_{p-1}\}$. This step can be performed by an external graph partitionner~\cite{} and is not adressed by this paper. As a data is mapped onto the mesh, the set of data $\Delta$ is partitionned the same way than the mesh in $\{\Delta_0,\dots,\Delta_{p-1}\}$. The second step of the parallelization is to identify in $\Gamma$ the needed synchronizations between resources to update data, and thus to build a new ordered list of computations $\Gamma_{data}$.

\begin{mydef}
For $n$ the number of computations in $\Gamma$, and for $i,j$ such that $i<j<n$, a \textit{synchronisation} is needed between $c_i$ and $c_j$, denoted $c_i \pprec c_j$, if $c_j=s_j(R_j,w_j,D_j,\text{exp}_j)$, and $w_i \subset R_j$. Moreover, the data to update is $w_i \cap R_j = w_i$.
\end{mydef}

Actually, a synchronization can only be needed by the data read by a stencil computation (not local), and only if this data has been modified before, which means that it has been written before. This synchronization is needed because the neighborhood function $\mathcal{N}$ of a stencil computation involves values computed on different resources.

\begin{mydef}
A synchronization between two computations $c_i \pprec c_j$ is defined as a specific computation $\text{update}(w_i \cap R_j)=\text{update}(w_i)$.
\end{mydef}

\begin{mydef}
An \textit{updated-computation} a computation which needs a data synchronization before it can be performed. If $c_i \pprec c_j$, with $i<j$, the computation $c_j$ is transformed to an updated-computation $c^*_j(R_j,w_j,D_j,\text{exp}_j,\text{update}(w_i))$ such that $\text{update}(w_i)$ is performed before the evaluation of $exp_j$. The updated-computation can also be denoted as $c^*_j(c_j,\text{update}(w_i))$.
\end{mydef}

\begin{mydef}
The concatenation of two ordered lists of respectively $n$ and $m$ computations $l_1=[c_i]_{0 \leq i \leq n-1}$ and $l_2=[c'_i]_{0 \leq i \leq m-1}$ is denoted $l_1 \cdot l_2$ and is equal to a new ordered list $l_3=[c_0,\dots,c_{n-1},c'_0,\dots,c'_{m-1}]$.
\end{mydef}

\begin{mydef}
From the ordered list of computation $\Gamma$, a new ordered list $\Gamma_{data}$ is obtained from the call $\Gamma_{data} = T_{data}(\Gamma,0)$, where $T_{data}$ is the recursive function defined as
\begin{equation*}
T_{data}(\Gamma,i) = 
\begin{cases} 	[\Gamma[i]] \cdot T_{data}(\Gamma,i+1) & \mbox{if }\forall j\leq i\mbox{, } c_j \not\pprec c_i\\
				[c^*_i(\Gamma[i],\text{update}(\{w_k\})] \cdot T_{data}(\Gamma,i+1) & \forall k < i\mbox{, }c_k \pprec c_i\\
				[] & \mbox{if }i=|\Gamma|.
\end{cases}
\end{equation*}
\end{mydef}

The construction of $\Gamma_{data}$ implies that if a synchronization is needed it is always proceeded before the computation wich needs it. However, an optimization is possible to combine synchronizations together, and thus to synchronize more than one data at a time. If the number of synchronizations is reduced, final performance of the program will be better. For this reason, we define a reduction to apply to $\Gamma_{data}$, and which is performed as a post-transformation.

\begin{myprop}
Denoting four computations $c_l$, $c_k$, $c_i$ and $c_j$ of $\Gamma$, with $l<k$, $i<j$, and where $c_l \pprec c_k$ and $c_i \pprec c_j$. Then, computations $c_k$ and $c_j$ are tansformed to $c^*_k(c_k,update(w_l)$ and $c^*_j(c_j,update(w_i)$. If $i<k$, a single update call can be performed at $k$ such that $c^*_k(c_k,update(w_l\cup w_i)$, and in this case, the computation $c_j$ is not transformed.
\end{myprop}

\begin{proof}
By definition, the synchronization of $w_i$ can be performed between $c_i$ and $c_j$. As $i<k$ the computation $c_i$ is not performed between $k$ and $j$. As a result, the synchronization of $w_i$ can be performed between $c_i$ and $c_k$.
\end{proof}

In other words, for each synchronization in $\Gamma_{data}$, if the computation $c_i$ from which $c_j$ is synchronized occurs before the last synchronization, the two synchronizations can be reduced to one with the union of the data to update.


 The final step of this parallelization is to run $\Gamma_{data}$ on each resource. Thus, for each resource $0 \leq k \leq p-1$ a multi-stencil program defined by
\begin{equation}
\mathcal{MSP}_k(T,\mathcal{M}_k,\Delta_k,\Gamma_{data}),
\end{equation}
is performed.

We denote this parallelization technique a coarse-grain data parallelization in contrast with the same technique applied to the finer level of a single computation. In this case, for a computation $c(R,w,D,\text{exp})$, the local domain $D$ is partitionned for $p$ resources $\{D_0,\dots,D_{p-1}\}$, and each resource $k$ is responsible for a sub-computation $c_k(R,w,D_k,\text{exp}')$.
%This finer data parallelization is particularly adapted to shared memory architectures (multi-cores, many-cores or GPUs), and in such case it does not require the addition of synchronizations because the computed element is local to the sub-domain $D_k$. Actually, if the computation is a stencil, the computation only read neighborhood elements but does not write them which does not require a synchronization, and if the computation is local nothing is read or written in the sub-domain of another resource. 
This finer data parallelization technique is not directly adressed by the work presented in this paper, but is exhibited, as it will be explained later. 

%------------------------------
\subsection{Task parallelism}
A task parallelization technique is a technique to transform a program as a dependency graph of different tasks. A dependency graph exhibits parallel tasks, or on the contrary sequential execution of tasks. In a multi-stencil program a task is defined as a computation $c(R,w,D,\text{exp})$. As a result, the dependency graph of a multi-stencil program represents the dependencies between local and stencil computations of $\Gamma$.

\begin{mydef}
For two computations $c_i(R_i,w_i,D_i,e_i)$ and $c_j(R_j,w_j,D_j,e_j)$, with $i < j$, it is said that $c_j$ is data dependant from $c_i$, denoted $c_i\prec c_j$, if $w_i \cap R_j \neq \emptyset$. In this case, $c_i$ has to be computed before $c_j$. The binary relation $c_i\prec c_j$ represents a \textit{dependency}.
\end{mydef}

\begin{myprop}
The binary relation $\prec$ is not transitive. 
\end{myprop}

\begin{proof}
Considering three computations $c_0(R_0,w_0,D_0,e_0)$, $c_1(R_1,w_1,D_1,e_1)$ and $c_2(R_2,w_2,D_2,e_2)$, where $w_0\cap R_1 \neq \emptyset$ and $w_1\cap R_2 \neq \emptyset$. In this case two dependencies can be extracted $c_0 \prec c_1$ and $c_1 \prec c_2$. However, the dependency $c_0 \prec c_2$ is not true as $w_0\cap R_2 = \emptyset$.
\end{proof}

It has been proved that the binary relation $\prec$ is not transitive. This property is due to the fact that $\prec$ is defined as a data dependency between computations. A dependency chain, however, creates another type of dependency only due to time.

\begin{mydef}
For two computations $c_i(R_i,w_i,D_i,e_i)$ and $c_j(R_j,w_j,D_j,e_j)$, a time dependency is denoted $c_i \blacktriangleleft c_j$ and means that $c_i$ has to be computed before $c_j$.
\end{mydef}

As a result, the relation $\blacktriangleleft$ is more general than $\prec$.

\begin{myprop}
For two computations $c_i(R_i,w_i,D_i,e_i)$ and $c_j(R_j,w_j,D_j,e_j)$ such that $c_i \prec c_j$, $c_i \blacktriangleleft c_j$ is verified.
\end{myprop}

\begin{proof}
By definition $c_i \prec c_j$ means that $w_i\cap R_j \neq \emptyset$ and that $c_i$ has to computed before $c_j$. %Thus, the relation $c_i \prec c_j$ is stronger than $c_i \blacktriangleleft c_j$.
\end{proof}

\begin{myprop}
The binary relation $\blacktriangleleft$ is transitive. 
\end{myprop}

\begin{proof}
Considering three computations $c_0(R_0,w_0,D_0,e_0)$, $c_1(R_1,w_1,D_1,e_1)$ and $c_2(R_2,w_2,D_2,e_2)$, where $c_0 \blacktriangleleft c_1$ and $c_1 \blacktriangleleft c_2$. $c_0$ is computed before $c_1$ and $c_1$ is computed before $c_2$, as a result $c_0$ is computed before $c_2$ and the relation $c_0 \blacktriangleleft c_2$ is verified. 
\end{proof}

\begin{mydef}
A directed acyclic graph (DAG) $G(V,A)$ is a graph where the edges are directed from a source to a destination vertex, and where following the direction of edges, no cycle can be found from a vertex $u$ to itself. A directed edge is called an arc, and for two vertices $v,u \in V$ an arc from $u$ to $v$ is denoted $(\overset{\frown}{u,v}) \in A$.
\end{mydef}

From an ordered list of computations, a directed dependency graph $\Gamma_{task}(V,A)$ can be built finding all pairs of computations $c_i(R_i,w_i,D_i,e_i)$ and $c_j(R_j,w_j,D_j,e_j)$, with $i<j$, such that $c_i \blacktriangleleft c_j$. Those time dependencies can be found from pairs of data dependencies $c_i \prec c_j$.

\begin{mydef}
For two directed graphs $G(V,A)$ and $G'(V',A')$, the union $(V,A)\cup (V',A')$ is defined as the union of each set $(V\cup V', A \cup A')$.
\end{mydef}

\begin{mydef}
From an ordered list $\Gamma$ of computations $c(R,w,D,\text{exp})$, a directed dependency graph $\Gamma_{task}(V,A)$ is obtained from the call $T_{task}(\Gamma,0)$, where $T_{task}$ is the recursive function
\begin{equation*}
T_{task}(\Gamma,i) = 
\begin{cases} 	(\{\},\{\}) & \mbox{if }i=|\Gamma|\\
				(c_i, \{(\overset{\frown}{c_k,c_i})\mbox{, }\forall k < i \mbox{, } c_k\prec c_i \})\cup T_{task}(\Gamma,i+1) & \mbox{if }i<|\Gamma|
\end{cases}
\end{equation*}
\end{mydef}

This constructive function is possible because the input is an ordered list. Actually, if $c_k\prec c_i$ then $k<i$. As a result, $c_k$ is already in $V$ when the arc $(\overset{\frown}{c_k,c_i})$ is built. 

\begin{myprop}
The directed graph $\Gamma_{task}$ is an acyclic graph.
\end{myprop}

\begin{proof}
$\Gamma_{task}$ is built from $\Gamma$ which is an ordered and sequential list of computations. Moreover, each computation of the list $\Gamma$ is associated to a vertex of $V$, even if the same computation is represented more than once in $\Gamma$. As a result it is not possible to go back to a previous computation and to create a cycle.
\end{proof}

Using the function $T_{task}$ to build $\Gamma_{task}$, however, duplication of dependencies may occur because of the transitivity of the relation $\blacktriangleleft$. Actually, as the relation $\prec$ verifies the relation $\blacktriangleleft$, and as $\blacktriangleleft$ is transitive, if for three computations $c_k(R_k,w_k,D_k,e_k)$, $c_i(R_i,w_i,D_i,e_i)$ and $c_j(R_j,w_j,D_j,e_j)$, with $k<i<j$ and where $c_k \prec c_i$, $c_i \prec c_j$ and $c_k \prec c_j$, then the transitivity $c_k \blacktriangleleft c_j$ is verified because of the relations $c_k \blacktriangleleft c_i$ and $c_i \blacktriangleleft c_j$. However, $c_k \blacktriangleleft c_j$ is also directly represented by the binary relation $c_k \prec c_j$. As a result, a duplication of the dependency is created in $\Gamma_{task}$. In Figure~\ref{fig:duplication} an example of duplication is given for the relation $c_1 \blacktriangleleft c_4$.

\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   \node[] (c0) at (0,0) {$c_0$};
   \node[] (c1) at (1,0) {$c_1$};
   \node[] (c2) at (2,0) {$c_2$};
   \node[] (c3) at (3,0) {$c_3$};
   \node[] (c4) at (4,0) {$c_4$};
 
  \path[->]
    (c0) edge node {} (c1)
    (c1.east) edge node {} (c2)
    	 edge [bend right=50] node [swap] {} (c4)
    (c2) edge node {} (c3)
    (c3) edge node {} (c4);
  \end{tikzpicture}
  \caption{Useless duplication of the dependency $c_1 \blacktriangleleft c_4$}
  \label{fig:duplication}
\end{center}
\end{figure}

Another view of the relations $\prec$ and $\blacktriangleleft$ is that $c_i\blacktriangleleft c_j$ in $\Gamma_{task}$ is a path from $c_i$ to $c_j$ of any size, while $c_i \prec c_j$ is a path of size $1$ from $c_i$ to $c_j$. And in any case a path in the graph represents a time dependency between computations. Scheduling a graph dependency which do not contains duplication of information is easier. 
%For this reason, we define a reduction to apply to $\Gamma_{task}$, and which is performed as a post-transformation. In the example of Figure~\ref{fig:duplication} the dependency represented by the path $c_1 \blacktriangleleft c_2 \blacktriangleleft c_3 \blacktriangleleft c_4$ gives more information than the one directly represented by $c_1 \prec c_4$. The proposed reduction is based on this property.

% \begin{mydef}
% The dependency graph $\Gamma_{task}(V,A)$ is reduced to the dependency graph $\Gamma_{task}(V,A')$, where $A' \subseteq A$. A direct arc $(\overset{\frown}{c_i,c_j})$, $i<j$, of $A$, which means that $c_i \prec c_j$, is included in $A'$ if no longer path $c_i \blacktriangleleft c_j$ exists in $A$ because of transitivity.
% \end{mydef}
\begin{mydef}
A DAG is transitive if it contains an arc $(\overset{\frown}{u,v})$ between any two vertices such that there is a path from $u$ to $v$. The transitive closure of a DAG $G=(V,E)$, is the DAG $G_T=(V,E_T)$ for which $E_T$ is the minimal subset of $V \times V$ that includes $E$ and makes $G_T$ transitive.
\end{mydef}

\begin{mydef}
An arc $(\overset{\frown}{u,v})$ of a DAG is redundant if there is a path from $u$ to $v$ in the DAG that does not include the edge. A DAG that does not contain any redundant arc is called minimal. The transitive reduction of a DAG $G$ is its unique minimal sub-graph having the same transitive closure.
\end{mydef}

Thus, the removal of the redundant dependencies of $\Gamma_{task}$ consists in applying a transitive reduction.

%------------------------------
\subsection{Hybrid parallelism}
It is also possible to combine coarse-grain data and task parallelization techniques to get hybrid parallelism, sometimes more efficient on hybrid architectures. A coarse-grain data parallelism creates for $k$ resources $k$ multi-stencil computations $\mathcal{MSP}_k(T,\mathcal{M}_k,\Delta_k,\Gamma_{data})$, where $\Gamma_{data}$ is built from an ordered list of computations and is itself an ordered list of computations $c$ and updated-computations $c^*$. On the other hand, the task parallelization technique builds, from an ordered list of computation, a graph dependency $\Gamma_{task}$. As a result $T_{data}$ and $T_{task}$ can be composed to build $\Gamma_{hybrid}$. Thus, $k$ multi-stencil programs
\begin{equation*}
\mathcal{MSP}_k(T,\mathcal{M}_k,\Delta_k,\Gamma_{hybrid})
\end{equation*}
are responsible for an hybrid parallelization of a multi-stencil program. The set of computation $\Gamma_{hybrid}$ is a dependency graph between computations $c$ and updated-computations $c^*$ and can be built from the call to 
\begin{equation*}
T_{task}(T_{data}(\Gamma,0),0).
\end{equation*}

%------------------------------
\subsection{Series-parallel graphs and series-parallel tree decomposition}
In this section we argue that the graphs $\Gamma_{task}$ or $\Gamma_{hybrid}$, previously defined, on which an approximation will be defined, are minimal series-parallel graphs. For this reason, the structure of those graphs can be represented as binary trees of parallel and series compositions of sub-graphs, also called \emph{binary decomposition tree}~\cite{Valdes:1979:RSP:800135.804393}. First, the needed definitions on series-parallel graphs will be given...

\paragraph{GSP and MSP classes.} 
A vertex $v$ of a DAG $G$ is a \emph{source} if no edge of $G$ enters $v$. Similarly, a vertex $v$ is a \emph{sink} if no edge of $G$ leaves $v$. In 1982, Valdes \& Al~\cite{Valdes:1979:RSP:800135.804393} have defined the class of minimal series-parallel DAGs (MSP).

\begin{mydef}Minimal Series Parallel
\begin{itemize}
\item The DAG having a single vertex and no edges is MSP.
\item If $G_1=(V_1,E_1)$ and $G_2=(V_2,E_2)$ are two MSP DAGs, so is either of the DAGs constructed by the following operations:
\begin{itemize}
\item Parallel composition: $G_p=(V_1\cup V_2,E_1\cup E_2)$.
\item Series composition: $G_s=(V_1\cup V_2,E_1\cup E_2\cup (N_1 \times R_2))$, where $N_1$ is the set of sinks of $G_1$ and $R_2$ is the set of sources of $G_2$.
\end{itemize}
\end{itemize}
\end{mydef}

\begin{mydef}
A DAG is \emph{General Series Parallel} (GSP) if and only if its transitive reduction is a MSP DAG.
\end{mydef}

A \emph{binary decomposition tree} is a tree having a leaf for each vertex of the MSP DAG it represents, and whose internal nodes are labelled $S$ or $P$ to indicate respectively the series or parallel composition of the MSP sub-DAGs represented by the subtrees rooted at $S$ or $P$. Figures~\ref{fig:gsp}, ~\ref{fig:msp} and~\ref{fig:t} respectively give an example of a GSP DAG, its transitive reduction which is MSP, and its tree decomposition.

\begin{figure}[h!]
\begin{center}
\subfigure[][\label{fig:gsp}]{
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   \node[] (a) at (0,0) {$a$};
   \node[] (b) at (1,0) {$b$};
   \node[] (c) at (2,0.5) {$c$};
   \node[] (d) at (3,0.5) {$d$};
   \node[] (e) at (2,-0.5) {$e$};
   \node[] (f) at (3,-0.5) {$f$};
   \node[] (g) at (4,0) {$g$};
   \node[] (h) at (1,-1) {$h$};
   \node[] (i) at (2,-1) {$i$};
   \node[] (j) at (3,-1) {$j$};
 
  \path[->]
    (a) edge node {} (b)
        edge [bend left=50] node [swap] {} (d)
        edge node {} (h)
    (b) edge node {} (c)
        edge node {} (e)
    (c) edge node {} (d)
        edge node {} (f)
    (e) edge node {} (d)
        edge node {} (f)
    (d) edge node {} (g)
    (f) edge node {} (g)
    (h) edge node {} (i)
        edge [bend right=50] node [swap] {} (j)
    (i) edge node {} (j);
  \end{tikzpicture}
}
\hspace{10pt}
\subfigure[][\label{fig:msp}]{
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
  \node[] (a) at (0,0) {$a$};
   \node[] (b) at (1,0) {$b$};
   \node[] (c) at (2,0.5) {$c$};
   \node[] (d) at (3,0.5) {$d$};
   \node[] (e) at (2,-0.5) {$e$};
   \node[] (f) at (3,-0.5) {$f$};
   \node[] (g) at (4,0) {$g$};
   \node[] (h) at (1,-1) {$h$};
   \node[] (i) at (2,-1) {$i$};
   \node[] (j) at (3,-1) {$j$};
 
  \path[->]
    (a) edge node {} (b)
        edge node {} (h)
    (b) edge node {} (c)
        edge node {} (e)
    (c) edge node {} (d)
        edge node {} (f)
    (e) edge node {} (d)
        edge node {} (f)
    (d) edge node {} (g)
    (f) edge node {} (g)
    (h) edge node {} (i)
    (i) edge node {} (j);
  \end{tikzpicture}
}
\end{center}
\caption{(a) GSP and (b) MSP DAGs}
\label{fig:gspmsp}
\end{figure}

\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   \node[] (S1) at (0,0) {$\mathcal{S}$};
   \node[] (P1) at (0.5,1) {$\mathcal{P}$};
   \node[] (a) at (-0.5,1) {$a$};
   \node[] (S2) at (-1,2) {$\mathcal{S}$}; %+1
   \node[] (S3) at (2,2) {$\mathcal{S}$}; %-1
   \node[] (b) at (-2,3) {$b$};
   \node[] (S4) at (0,3) {$\mathcal{S}$};
   \node[] (P2) at (-1,4) {$\mathcal{P}$};
   \node[] (g) at (1,4) {$g$};
   \node[] (S5) at (-2,5) {$\mathcal{S}$};
   \node[] (S6) at (0,5) {$\mathcal{S}$};
   \node[] (c) at (-2.5,6) {$c$};
   \node[] (d) at (-1.5,6) {$d$};
   \node[] (e) at (-0.5,6) {$e$};
   \node[] (f) at (0.5,6) {$f$};
   
   \node[] (h) at (1,3) {$h$};
   \node[] (S7) at (3,3) {$\mathcal{S}$};
   \node[] (i) at (2.5,4) {$i$};
   \node[] (j) at (3.5,4) {$j$};
 
  \path[->]
    (S1) edge node {} (a)
          edge node {} (P1)
    (P1) edge node {} (S2)
          edge node {} (S3)
    (S2) edge node {} (b)
        edge node {} (S4)
    (S4) edge node {} (P2)
          edge node {} (g)
    (P2) edge node {} (S5)
          edge node {} (S6)
    (S5) edge node {} (c)
          edge node {} (d)
    (S6) edge node {} (e)
          edge node {} (f)
    (S3) edge node {} (h)
         edge node {} (S7)
    (S7) edge node {} (i)
          edge node {} (j);
  \end{tikzpicture}
  \caption{Binary decomposition tree of the MSP of Figure~\ref{fig:msp}}
  \label{fig:t}
\end{center}
\end{figure}

Valdes \& Al~\cite{Valdes:1979:RSP:800135.804393} have also proposed a linear algorithm to know if a DAG is MSP, and if it is to decompose it to the associated series-parallel tree. This algorithm is based on the duality of the class of MSP DAGs, with the class of \emph{Two Terminal Series Parallel} DAGs (TTSP) from which a binary tree decomposition can be performed in linear time. Other algorithms have also been proposed, all of them in linear time~\cite{Schoenmakers95anew}.

Finally, Valdes \& Al~\cite{Valdes:1979:RSP:800135.804393} have identify a forbidden shape, or subgraph, called $N$ and represented in Figure~\ref{fig:n}.

\begin{mydef}
A DAG $G$ is GSP if and only if its transitive closure does not contain $N$ as a subgraph.
\end{mydef}

\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   \node[] (a) at (0,0) {$a$};
   \node[] (b) at (1,0) {$b$};
   \node[] (c) at (0,-1) {$c$};
   \node[] (d) at (1,-1) {$d$};
 
  \path[->]
    (a) edge node {} (b)
    (c) edge node {} (b)
        edge node {} (d);
  \end{tikzpicture}
  \caption{Forbidden $N$ subgraph shape for a DAG to be GSP.}
  \label{fig:n}
\end{center}
\end{figure}

\paragraph{Multi-stencil programs.}
We are interested in GSP and MSP classes of graphs to be able to represent computations of a multi-stencil program as a set of sequences or parallel executions. Actually, such a representation can directly be dumped to a parallel language, in which \emph{sequence} of instructions and \emph{parallel} execution of instructions are defined. Series-parallel trees can also be used as input of scheduling optimizations~\cite{Finta1996323,Wang20082684} to improve task parallelism efficiency. Finally, in this paper is presented that such a series-parallel representation can also be dumped to component models by defining specific control components. Thus, the proposed DSL inheritates, among others, some software engineering advantages of component models, such as code re-use, productivity and maintainability.

However, even by applying the transitive reduction on the computation DAG $\Gamma_{task}$ (or $\Gamma_{hybrid}$), it is possible to obtain a DAG not MSP. Actually, it is possible in a multi-stencil program to have a set of computations such that their dependencies form the forbidden $N$ subgraph. For example, if $c_0$, $c_1$, $c_2$ and $c_3$ are computations of a $\mathcal{MSP}$ such that $c_0 \prec c_1$, $c_2 \prec c_1$ and $c_2 \prec c_3$, the \emph{zigzag} relation $c_0 \prec c_1 \succ c_2 \prec c_3$ which form a forbidden subgraph of MSP DAGs is found in $\Gamma_{task}$. For this reason, we have made the choice to over-constrain such a case by adding the relation $c_0 \prec c_3$ such that a complete graph is created and can be translated to a series-parallel decomposition as illustrated in Figure~\ref{fig:allover}.

\begin{figure}[h!]
\begin{center}
\subfigure[][Over-constraint on the forbidden $N$ shape.\label{fig:over}]{
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   \node[] (c0) at (0,0) {$c_0$};
   \node[] (c1) at (2,0) {$c_1$};
   \node[] (c2) at (0,-1) {$c_2$};
   \node[] (c3) at (2,-1) {$c_3$};
 
  \path[->]
    (c0) edge node {} (c1)
          edge [dashed] node [swap] {} (c3)
    (c2) edge node {} (c1)
        edge node {} (c3);
  \end{tikzpicture}
}
\hspace{50pt}
\subfigure[][Series-parallel tree associated to the over-constraint\label{fig:treeover}]{
\begin{tikzpicture}[shorten >=1pt, node distance=2cm, on grid, auto]
   \node[] (P1) at (0,0) {$\mathcal{P}$};
   \node[] (S1) at (-1,1) {$\mathcal{S}$};
   \node[] (S2) at (1,1) {$\mathcal{S}$};
   \node[] (c0) at (-1.5,2) {$c_0$};
   \node[] (c1) at (-0.5,2) {$c_1$};
   \node[] (c2) at (0.5,2) {$c_2$};
   \node[] (c3) at (1.5,2) {$c_3$};
 
  \path[->]
    (P1) edge [dashed] node [swap] {} (S1)
          edge [dashed] node [swap] {} (S2)
    (S1) edge node {} (c0)
          edge node {} (c1)
    (S2) edge node {} (c2)
        edge node {} (c3);
  \end{tikzpicture}
}
\caption{Deletion of forbidden subgraphs.}
\label{fig:allover}
\end{center}
\end{figure}

This approximation on the dependencies is acceptable for multi-stencil programs, first because it rarely appends, second because of the relative homogeneity of computations. Actually, all computations, except the ones on the physical border of the domain, are performed on an entire domain of the mesh, and as a computation performs a single quantity at a time ($w_i$ of $c_i$ is a singleton), the amount of arithmetic operations in a computation are quite homogeneous.

After these over-constraints are applied, $\Gamma_{task}$ (or $\Gamma_{hybrid}$) is a MSP DAG. As a result, the binary series-parallel tree decomposition can be computed in linear time. At this point, the $\mathcal{MSP}$ program could be dumped to any parallel language in which a sequence and a parallel execution is available. For example, we could imagine a parallel functionnal language with a binary \emph{sequence} function and a binary \emph{parallel} function such that the tree of Figure~\ref{fig:t} corresponds the following nested call:

\begin{equation}
\begin{split}
\text{sequence}(a,\text{parallel}(&\\
&\text{sequence}(b,\text{sequence}(\text{parallel}(\text{sequence}(c,d),\text{sequence}(e,f)),g)),\\
&\text{sequence}(h,\text{sequence}(i,j))\\
&))
\end{split}
\end{equation}